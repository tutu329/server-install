【支付宝购买gpt订阅】
    1、appstore登录apple美id
    2、支付宝切换旧金山，搜索pockytshop，里面搜索apple(apple gift card us)，购买25$（注意复制礼品卡卡号），appstore中输入礼品卡卡号兑换
    3、开tz，gpt设置-订阅：取消订阅，然后续期。ok


【关于ubuntu系统突然掉显卡/突然无故重启的问题】
    1)突然掉显卡(nvidia-smi报错: unable to determine the device handle for GPU 0000.1A.00.0):
        查看gpu的总线情况lspci | grep NVIDIA
            如果像这样, rev a1结尾为正常, rev ff结尾为错误。     (1a:00.0 VGA compatible controller: NVIDIA Corporation TU102 [GeForce RTX 2080 Ti Rev. A] (rev a1))
        查看gpu id和bus_id的对应关系: nvidia-smi --query-gpu=index,pci.bus_id,uuid --format=csv
        查看报错日志(一般也不需要): sudo nvidia-bug-report.sh
    2)突然无故重启
        a)应该是功率问题，解决方式(gpu.sh):
            sudo nvidia-smi -pm 1 # 设置显卡持久模式
            sudo nvidia-smi -lgc 1400 # 固定显卡频率
            sudo nvidia-smi -pl 200 # 设置显卡最大功率
    3) 1和2的解决手段:
        a)重启
        b)运行llm前，先运行gpu.sh设置持久模式、控制功率、控制频率

【超微4029GR服务器的ipmi的bios配置，目前服务器ipmi的ip为192.168.124.125，用户名为admin，密码为jackseaver79】
1、让商家交货时，确保服务器的bios中设置如下:
    1) 已经打开远程登录ipmi的设置
    2) ipmi的ip最好设置为192.168.124.125这样，也就是和家里的网段前三个数字一致，省的家里机器为了访问同网段的ipmi要改ip
    3) 风扇模式已经设置为优化(optimize)模式
2、ubuntu中安装ipmitool
    安装ipmitool: sudo apt install ipmitool
    浏览器远程登录ipmi(如有问题，要进入服务器bios设置):
        将网线接入服务器的单独的那个impi网口
        浏览器打开192.168.124.125
        用户名为：admin，密码为：jackseaver79
    查看服务器电源情况:
        将网线接入服务器的单独的那个impi网口
        ipmitool -I lanplus -H 192.168.124.125 -U admin -P jackseaver79 sdr type "Power Supply"

【超微4029GR服务器的ubuntu系统安装】
1、显卡驱动
    安装535专用版(tested)，暂不考虑server版
    nvidia-smi后显示: NVDIA-SMI: 535.161.07 Driver Version: 535.161.07, CUDA Version: 12.2

2、安装gcc
    sudo apt update
    sudo apt install g++
    sudo apt install gcc
    sudo apt install make

3、cudatoolkit安装
    https://developer.nvidia.com/cuda-downloads选择runfile(local)
    wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run
    sudo sh cuda_12.4.0_550.54.14_linux.run
        1)等一段时间，选择continue，选择accept
        2)把Driver的选择去掉，然后选择install
    安装完成后: vi .bashrc
        export PATH="/home/tutu/anaconda3/bin:$PATH"
        export CUDA_HOME=/usr/local/cuda
        export CPATH=$CUDA_HOME/targets/x86_64-linux/include:$CPATH
        export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$CUDA_HOME/targets/x86_64-linux/lib:$CUDA_HOME/extras/CUPTI/lib64:$LD_LIBRARY_PATH
        export PATH=$PATH:$LD_LIBRARY_PATH:$CUDA_HOME:$CUDA_HOME/bin:$CUDA_HOME/nvvm/bin
    source .bashrc
    nvcc -V
    安装成功

4、安装vllm
    conda create -n v1 python=3.11
    conda activate v1
    pip install torch torchvision torchaudio
    pip install vllm(似乎前面cudatoolkit选择12.4也可以)

4.1、vllm0.4.0需要安装nccl(https://developer.nvidia.com/nccl找对应自己cuda版本的nccl)
    下载https://developer.nvidia.com/downloads/compute/machine-learning/nccl/secure/2.20.5/ubuntu2204/x86_64/nccl-local-repo-ubuntu2204-2.20.5-cuda12.4_1.0-1_amd64.deb
    sudo dpkg -i nccl-local-repo-ubuntu2204-2.20.5-cuda12.4_1.0-1_amd64.deb
    sudo apt update
    sudo apt install libnccl2 libnccl-dev （也可以安装指定版本sudo apt install libnccl2=2.4.8-1+cuda10.0 libnccl-dev=2.4.8-1+cuda10.0）
4.2、安装flash-attention 1.0.9
    1）vi .bashrc (补充下述路径)
        export PATH="/home/tutu/anaconda3/bin:$PATH"
        export CUDA_HOME=/usr/local/cuda
        export CPATH=$CUDA_HOME/targets/x86_64-linux/include:$CPATH
        export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$CUDA_HOME/targets/x86_64-linux/lib:$CUDA_HOME/extras/CUPTI/lib64:$LD_LIBRARY_PATH
        export PATH=$PATH:$LD_LIBRARY_PATH:$CUDA_HOME:$CUDA_HOME/bin:$CUDA_HOME/nvvm/bin
    2）source .bashrc
    3）pip install flash-attn==1.0.9
    注意报错，如：“ld: cannot find -lcudart: 没有那个文件或目录“，通常就是上述路径设置有问题

5、exfat安装
    安装exfat支持: sudo apt install exfatprogs
    mount移动硬盘: sudo mount -t exfat /dev/sda2 /mnt/disk_1t/
    复制模型文件等

6、安装git
    sudo apt install git
    ssh -T git@github.com, 如果看到Warning: Permanently added 'github.com' ...Permission denied..., 表面可以连接
    sudo apt install git-lfs
    git lfs install
    这样就可以下载了

7、安装frpc及其自启动、打开ssh(决定了impi让机器重启后, 外网能否ssh连接到机器)
    frpc参照life-agent中的frpc配置来即可(详细可以参考: https://blog.csdn.net/ludan1991/article/details/129397133)
    frpc在系统启动时自动启动:
        sudo vi /etc/systemd/system/frpc.service

[Unit]
Description=frpc Service
After=network.target
Wants=network.target

[Service]
Type=simple
User=tutu
Restart=always
RestartSec=5s
ExecStart=/home/tutu/frp/frpc -c /home/tutu/frp/frpc.ini
ExecReload=/home/tutu/frp/frpc -c /home/tutu/frp/frpc.ini

[Install]
WantedBy=multi-user.target

        sudo systemctl daemon-reload
        sudo systemctl start frpc.service
        sudo systemctl status frpc.service
        sudo systemctl enable frpc.service

    打开ssh(注意frp中22端口映射到了6000):
        sudo apt install openssh-server
        sudo service ssh start (sudo service ssh status)
        sudo systemctl enable ssh (sudo systemctl is-enabled ssh)
        sudo ufw allow ssh (如果需要专门打开防火墙的话)
        sudo apt install net-tools (用于ifconfig)
        其他电脑上可以(端口为6000, 密码为jackseaver79): ssh -p 6000 tutu@116.62.63.204

8、安装jupyterlab及其自启动
    1)安装:
        conda create -n ju python=3.11
        pip install jupyterlab
    2)启动sh:
        source /home/tutu/anaconda3/etc/profile.d/conda.sh
        conda activate ju
        jupyter lab --allow-root --ServerApp.allow_remote_access=True --ServerApp.token=Jackseaver112279 --ip=* --port=7862
    3)自启动:
        找到jupyter:
            conda activate ju
            whereis jupyter-lab ( /home/tutu/anaconda3/envs/ju/bin/jupyter )
        sudo vi /etc/systemd/system/jupyter.service

[Unit]
Description=jupyter lab
After=network.target
[Service]
Type=simple
User=tutu
ExecStart=/home/tutu/anaconda3/envs/ju/bin/jupyter lab --allow-root --ServerApp.allow_remote_access=True --ServerApp.token=Jackseaver112279 --ip=* --port=7862
ExecReload=/home/tutu/anaconda3/envs/ju/bin/jupyter lab --allow-root --ServerApp.allow_remote_access=True --ServerApp.token=Jackseaver112279 --ip=* --port=7862
ExecStop=/usr/bin/pkill /home/tutu/anaconda3/envs/ju/bin/jupyter
KillMode=process
Restart=on-failure
RestartSec=15s
[Install]
WantedBy=multi-user.target

        sudo systemctl daemon-reload
        sudo systemctl start jupyter.service
        sudo systemctl status jupyter.service
        sudo systemctl enable jupyter.service

    4)notebook更换conda对应的kernel
        1）对应conda环境下：pip install ipython ipykernel
        2）对应conda环境下：python -m ipykernel install --user --name ju_kernel (需要显示的kernel名字)
    5)解决ubuntu下notebook生成的图中中文显示为方块的问题
        安装字体
            git clone https://github.com/tracyone/program_font
            cd program_font
            ./install.sh
            fc-list :lang=zh | grep "SimHei"

        找到位置fonts所在位置
            import matplotlib
            matplotlib.matplotlib_fname() (返回'/home/tutu/anaconda3/envs/ju_kernel/lib/python3.10/site-packages/matplotlib/mpl-data/matplotlibrc')
            有其他字体可以复制到/home/tutu/anaconda3/envs/ju_kernel/lib/python3.10/site-packages/matplotlib/mpl-data/fonts下
        删除matplotlib缓冲
            cd ~/.cache/matplotlib
            rm -rf *.*
        python代码修改字体
            font_name = "simhei"
            plt.rcParams['font.family']= font_name      # 指定字体，实际上相当于修改 matplotlibrc 文件　只不过这样做是暂时的　下次失效
            plt.rcParams['axes.unicode_minus']=False    # 正确显示负号，防止变成方框
        notebook中生成的图的中文就正常了

8、安装agent用的chrome
    playwright install chrome

9、安装与/home/tutu/server/关联的ftp服务
    安装: sudo apt install vsftpd
    查询状态: service vsftpd status
    sudo service vsftpd start
    sudo service vsftpd restart
    sudo service vsftpd stop

    用户设置（虚拟user，一个linux的user("ftpuser")对应多个vsftp的虚拟user）:
        sudo useradd -d /home/tutu/server -s /sbin/nologin ftpuser  # ftpuser为新增的user，而虚拟用户对应/etc/vsftpd_user_conf/的tutu329等文件
        sudo chmod -Rf 777 /home/tutu/server    # 这一行很关键，不然filezilla上传下载会报错：严重文件传输错误
    conf配置
        sudo vi /etc/vsftpd.conf
        write_enable=YES
        local_enable=YES
        # chroot_local_user最好还是关闭
        # local_umask=022 # 这个打开才能上传(022上传后文件权限均为600，所以要改为000,但此时仍然是600，经测试必须设置anon_umask=000)
        local_umask=000
        anon_umask=000  # 注意：这行很重要，因为配置的是虚拟user，vsftpd中虚拟用户与匿名用户有相同的权限(virtual_use_local_privs=NO)
        pasv_enable=YES
        pasv_min_port=8020
        pasv_max_port=8025
        pasv_address=116.62.63.204  (这里最好设置pasv_address=0.0.0.0，否则局域网机器无法访问，只能通过frp，上传大文件很费流量)
        allow_writeable_chroot=YES  # 这行很关键，不然就连不上

        注: cat /etc/ftpusers为禁止作为ftp用户的用户列表
    创建虚拟用户
        sudo apt install db-util
        vi login_users (增加第一行: tutu329，第二行密码: jackseaver79)
        sudo db_load -T -t hash -f /home/tutu/login_users /etc/vsftpd_login.db
        sudo chmod 600 /etc/vsftpd_login.db

    配置PAM验证
        sudo vi /etc/pam.d/vsftpd (增加如下两行，其余注释掉:
            auth required /lib/x86_64-linux-gnu/security/pam_userdb.so db=/etc/vsftpd_login
            account required  /lib/x86_64-linux-gnu/security/pam_userdb.so  db=/etc/vsftpd_login
        )
        sudo vi /etc/vsftpd.conf (改为:
            pam_service_name=vsftpd
        )

    创建本地用户映射
        sudo vi /etc/vsftpd.conf (增加:
            guest_enable=YES
            guest_username=ftpuser
        )

    设置用户目录和权限
        sudo chown ftpuser /home/tutu/server/   # 注意这里是ftpuser
        sudo mkdir /etc/vsftpd_user_conf
        sudo vi /etc/vsftpd.conf (增加:
            user_config_dir=/etc/vsftpd_user_conf
        )

    新建某个虚拟user(如tutu329)的配置: 在/etc/vsftpd_user_conf/下创建文本文件tutu329:
        write_enable=YES
        anon_upload_enable=YES
        anon_mkdir_write_enable=YES

        anon_world_readable_only=NO
        anon_upload_enable=YES
        anon_mkdir_write_enable=YES
        anon_other_write_enable=YES
        local_root=/home/tutu/server/

    sudo service vsftpd restart
    此时用filezilla可以成功登录、下载、上传
        192.168.124.33:21   tutu329:jackseaver79    （尽可能用这个，不然需要消耗frp服务器流量，非常贵）
            如果报错：无法建立数据连接: WSAEADDRNOTAVAIL - 无法分配请求的地址，则把filezilla中的这个用户设置中的“传输设置”改为主动即可。
        116.62.63.204:6021  tutu329:jackseaver79
    如果外网filezilla可以成功登录、下载、上传，而pycharm连接不了ftp，应该是pycharm版本问题、需要用新版本

10、安装docker(生产环境中不要用非root账户运行docker)
    1)安装必要的证书并允许apt包管理器使用以下命令通过https使用存储库: sudo apt install apt-transport-https ca-certificates curl software-properties-common gnupg lsb-release
    2)添加Docker的官方GPG密钥: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    2)添加阿里的镜像源GPG密钥
        curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
        echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

    3)添加Docker官方库: sudo echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    4)更新Ubuntu源列表: sudo apt update
    5)安装Docker: sudo apt install docker-ce docker-ce-cli containerd.io docker-compose-plugin
    6)验证docker状态: systemctl status docker
    7)配置使Docker服务开机自启动: sudo systemctl enable docker
    8)查看docker版本: sudo docker version
    9)docker测试:
        sudo docker run hello-world
        sudo docker ps -a
    10)安装docker-compose:
        从github上下载docker-compose二进制文件安装:
            sudo curl -L "https://github.com/docker/compose/releases/download/v2.6.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
        加可执行权限并移动到bin目录下:
            sudo chmod +x /usr/local/bin/docker-compose
            docker-compose -v
            此时docker-compose已经可用，可以在如Flowise/docker/下运行: sudo docker-compose up -d 或 sudo docker-compose stop
    11)查看某个docker容器的实时输出:
        sudo docker logs -f m3e-large-api (这个退出不会导致容器退出)
        sudo docker attach 162a5f83ec60 (这是某个docker容器的ID)

    12)建立自定义的给agent沙箱用的docker image(jupyter_with_common_python_libs)
        1、准备文件：dockerfile（docker build需要的默认名字）
FROM codeocean/jupyterlab
WORKDIR /usr/local/jupyter_temp/
RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
RUN pip install pip -U			# RUN为docker build时运行内容
RUN pip install matplotlib
RUN pip install scikit-learn
RUN pip install pandas
RUN pip install tabulate
RUN pip install statsmodels
RUN pip install numpy
EXPOSE 7865
CMD ["jupyter", "lab", "--allow-root", "--ServerApp.allow_remote_access=True",  "--ServerApp.token=Jackseaver112279", "--no-browser", "--ip=*", "--port=7865"]
        2、dockerfile所在文件夹下，编译image
            docker build -t jupyter_with_common_python_libs .
            a)-t 参数用于指定新镜像的名称和可选的 tag
            b). 表示 Dockerfile 文件所在的当前目录，Docker 将从这个目录开始构建镜像。
        3、新建批处理并运行image（装载/home/tutu/jupyter_temp为docker下的/usr/local/jupyter_temp，用作数据交换）
            mkdir ~/jupyter_temp
            docker run -p 7865:7865 -v /home/tutu/jupyter_temp:/usr/local/jupyter_temp -it jupyter_with_common_python_libs(这个image运行时)
            win: docker run -p 7865:7865 -v /d/jupyter_temp:/usr/local/jupyter_temp -it jupyter_with_common_python_libs
        4、python安装docker库，并且增加docker权限（否则报错：Error while fetching server API version: ('Connection aborted.', PermissionError(13, 'Permission denied'))）
            安装：
                pip install docker
            允许tutu(如远程jupyter下)连接docker:
                sudo groupadd docker
                sudo usermod -aG docker $USER
                newgrp docker(更新组)
                测试（python里可以直接调用指定image，不需要docker运行如jupyter_with_common_python_libs镜像）：python life-agent/tools/exec_code/exec_python_linux.py


11、安装one-api
    git clone https://github.com/songquanpeng/one-api
    cd one-api
    设置端口: docker-compose.yml中one-api的第一个3000改为8002
    启动one-api: sudo docker-compose up -d, 报错: invalid interpolation format for services.one-api.healthcheck.test.[]: "wget -q -O - http://localhost:3000/api/status | grep -o '\"success\":\\s*true' | awk -F: '{print $2}'". You may need to escape any $ with another $
        解决: docker-compose.yml中test这行改为, test: ["CMD-SHELL", "curl -s http://localhost:3000/api/status | tee /dev/stderr | grep '\"success\":\\s*true' || exit 1"]
    设置one-api:
        1)登录和密码修改: root:123456登录，然后修改为root:jackseaver79
        2)增加渠道(增加llm):
            类型:         自定义渠道
            Base URL:    http://192.168.124.32:7862/lab
            名称:         qwen72
            模型:         qwen72, gpt-4(通过"输入自定义模型名称"依次输入)
            模型重定向(必须严格，否则报错):
{
  "gpt-4": "qwen72",
  "qwen72": "qwen72"
}
            密钥:         empty
        3)增加令牌:
            名称: aiServer
            过期时间: 永不过期
            额度: 无限额度
        4)渠道界面中点击"测试"
        5)令牌界面中点击"复制", 获取api-key
        6)flowise中，ChatOpenAI参数为:
            Connect Credential: aiServer (令牌名称)
            Model Name: gpt-4 (模型名称)
            BasePath: http://116.62.63.204:8002/v1  (注意这里是8002)

12、启动所有docker-compose的sh(-d是指后台运行)
    sudo docker-compose -f /home/tutu/Flowise/docker/docker-compose.yml up -d
    sudo docker-compose -f /home/tutu/one-api/docker-compose.yml up -d
    sudo docker run -p 8000:6008 --name m3e-large-api -d registry.cn-hangzhou.aliyuncs.com/fastgpt_docker/m3e-large-api

13、安装带api的m3e
    参考https://doc.fastgpt.in/docs/development/custom-models/m3e/
    1)拉取: sudo docker pull registry.cn-hangzhou.aliyuncs.com/fastgpt_docker/m3e-large-api
    2)运行: sudo docker run -p 8000:6008 --name m3e-large-api -d registry.cn-hangzhou.aliyuncs.com/fastgpt_docker/m3e-large-api
    3)one-api中添加渠道(注意one-api点击测试不行，因为one-api会调用/v1/chat/completions而不是/v1/embeddings):
        类型: 自定义渠道
        Base Url: http://116.62.63.204:8000 (注意这里是m3e的api的端口8000)
        名称: m3e_api
        模型: m3e
        密钥: sk-aaabbbcccdddeeefffggghhhiiijjjkkk
    4)在flowise中添加flow测试：
        a)添加OpenAI Embeddings Custom:
            Connect Credential: aiServer
            Model Name: m3e
            BasePath: http://116.62.63.204:8002/v1  (注意这里是one-api的端口8002)
        b)测试: 点击 Usert Vector Database

14、安装chroma
    1)git clone https://github.com/chroma-core/chroma.git
    2)cd chroma
    3)修改docker-compose.yml的services的ports为8004:8000
    4)sudo docker-compose up -d --build (网络不太好，多运行几次就行)
    5)查看docker容器flowise和chroma的输出:
        sudo docker logs -f m3e-large-api (这个退出不会导致容器退出，attach会导致容器退出)
        sudo docker attach 162a5f83ec60 (这是flowise的docker container ID)
        sudo docker attach 376b547be7e6 (这是chroma的docker container ID)
    6）查看chroma的docker容器的ip:
        sudo docker inspect 376b547be7e6 | grep "IPAddress" ("IPAddress": "192.168.16.2")
    7)flowise中, chroma卡的url填写: http://192.168.16.2:8000

15、(flowise+milvus尚未成功)安装运行 milvus-2.0.2 (确保frp已经打开8003端口)
    1)下载yml: wget https://github.com/milvus-io/milvus/releases/download/v2.0.2/milvus-standalone-docker-compose.yml -O docker-compose.yml
    2)mkdir milvus && cp docker-compose.yml milvus/ && cd milvus
    3)编辑yml:
        standalone的ports改为: "8003:19530"
    4)sudo docker-compose up -d (sudo docker-compose down)
    5)用pymilvus测试:
        conda create -n milvus python=3.10
        conda activate milvus
        pip install pymilvus==2.0.2 (pymilvus版本要和milvus版本一致)
        新建connect_milvus.py:
            from pymilvus import connections
            host = "116.62.63.204"
            port = 8003
            rtn=connections.connect(
              alias="default",
              host=host,
              port=port
            )
            print(f'pymilvus connect {host}:{port} : "{rtn}"')

            connections.disconnect("default")
        python connect_milvus.py

16、安装xinference(支持embeddings和openai api)
    1)下载m3e
        git clone https://www.modelscope.cn/Jerry0/M3E-large.git
    2)安装xinference并运行m3e
        conda create -n m3e python=3.10
        conda activate m3e
        pip install "xinference[all]"
        xinference-local -p 7870 (xinference-local --help解释很全)
            1）注册下自定义模型m3e，/home/tutu/models/M3E-large
            2）运行自定义模型m3e，模型名称为m3e (此时已经运行了openai格式的m3e的嵌入api)
            3）外部如flowise可以正常访问: http://116.62.63.204:7870/v1

17、安装最新node.js (node.js官网的download里找即可)
    1、curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash
    2、新开一个控制台 (或者)
    3、nvm install 20
    4、node -v
    5、npm -v

18、安装lobe-chat
    1)用docker启动，
        sudo docker run -d -p 7869:3210 \
          -e "OPENAI_API_KEY=empty" \
          -e "OPENAI_PROXY_URL=http://116.62.63.204:8001/v1" \
          -e "CUSTOM_MODELS=qwen72" \
          --name lobe-chat \
          --restart=always \
          lobehub/lobe-chat:latest
    2)注意设置的"语言模型"这里填写llm的proxy url后，"连接测试"报错不用管，主界面上方的角色要调整问刚刚设置的url对应角色才行

19、安装fastgpt(严格按官网要求用docker-compose启动)
    1）mkdir fastgpt && cd fastgpt
    2）curl -O https://raw.githubusercontent.com/labring/FastGPT/main/files/deploy/fastgpt/docker-compose.yml
    3）curl -O https://raw.githubusercontent.com/labring/FastGPT/main/projects/app/data/config.json
    4）修改docker-compose配置
        1）（这一条可以不考虑；如果改动需要重新docker-compose pull）docker-compose.yml 配置文件中 Mongo 为 5.x，部分服务器不支持，需手动更改其镜像版本为 4.4.24
        2）修改docker-compose.yml中fastgpt下的：
            ports:
                - 7863:3000
            environment:
                - OPENAI_BASE_URL=http://116.62.63.204:8001/v1
                - CHAT_API_KEY=empty

    5）启动docker容器
        cd fastgpt
        sudo docker-compose pull
        sudo docker-compose up -d

    6）初始化 Mongo 副本集(4.6.8以前可忽略)
        # 查看 mongo 容器是否正常运行
            sudo docker ps
        # 进入容器
            sudo docker exec -it mongo bash

        # 连接数据库
            mongo -u myusername -p mypassword --authenticationDatabase admin

        # 初始化副本集。如果需要外网访问，mongo:27017 可以改成 ip:27017。但是需要同时修改 FastGPT 连接的参数（MONGODB_URI=mongodb://myname:mypassword@mongo:27017/fastgpt?authSource=admin => MONGODB_URI=mongodb://myname:mypassword@ip:27017/fastgpt?authSource=admin）
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongo:27017" }
  ]
})
        # 检查状态。如果提示 rs0 状态，则代表运行成功
            rs.status()

    7）修改config.json中llmModels的模型（"model"值必须和vllm启动参数中的served-model-name一致）
      "llmModels": [
        {
          "model": "qwen72",
          "name": "qwen72",
          "maxContext": 32000,
          "maxResponse": 4000,
          "quoteMaxToken": 32000,
          "maxTemperature": 1,

          "inputPrice": 0,
          "outputPrice": 0,
          "censor": false,
          "vision": false,
          "datasetProcess": false,
          "toolChoice": true,
          "functionCall": false,
          "customCQPrompt": "",
          "customExtractPrompt": "",
          "defaultSystemChatPrompt": "",
          "defaultConfig": {}
        },

    8）访问fastgpt
        sudo docker-compose down
        sudo docker-compose up -d
        目前可以通过 ip:7863 直接访问(注意防火墙)。登录用户名为 root，密码为docker-compose.yml环境变量里设置的 DEFAULT_ROOT_PSW。

20、运行wechat-gpt
    config.json关键参数(用one-api做成gpt-4)：
        "openai_api_base": "http://116.62.63.204:8002/v1",
        "model": "gpt-4",
        "openai_api_key": "sk-dzU39WNBrCXSSd81A0Be0196D83c461dBbFfFe974e6bC0Ca",
        "chat_group_session_independent": true,
        "max_tokens": 20000,
        "temperature": 0,





21、安装梯子
    0) 安装后的测试:
        curl -x http://localhost:7890 https://www.google.com
        curl -x http://localhost:7890 https://huggingface.co
    1) mkdir v2ray && cd v2ray
    2) curl -O https://raw.githubusercontent.com/v2fly/fhs-install-v2ray/master/install-release.sh
    3) sudo bash install-release.sh
    4) 常用命令:
        sudo systemctl status v2ray
        sudo systemctl enable v2ray
        sudo systemctl restart v2ray
        sudo systemctl start v2ray
        sudo systemctl stop v2ray
        netstat -autn | grep 7890

        git config --global http.proxy http://localhost:7890
        git config --global https.proxy http://localhost:7890
        查看: git config --global -l
        git config --global --unset http.proxy
        git config --global --unset https.proxy

        export http_proxy="http://127.0.0.1:7890"
        export https_proxy="http://127.0.0.1:7890"

        a) python中，若playwright需要通过v2ray代理:
            1、搜索过程:
                增加proxy={"server": "http://127.0.0.1:7890"}:
                self.browser = await p.chromium.launch(
                    channel="chrome",
                    headless=True,
                    proxy={"server": "http://127.0.0.1:7890"},
                )   # 启动chrome
            2、获取搜索结果url内容的过程:
                尚未解决！

        b) 如果python程序中需要访问外网，上述变量可能不起作用，需采用以下方式在程序里写入这些，用于测试：
            import requests

            proxies = {
                'http': 'http://127.0.0.1:7890',
                'https': 'http://127.0.0.1:7890',
            }

            try:
                response = requests.get('https://www.google.com', proxies=proxies)
                print(response.status_code)
                print(response.text[:200])  # 打印前200个字符
            except requests.exceptions.RequestException as e:
                print(e)

    5) sudo vi /usr/local/etc/v2ray/config.json (这里只放了个美国4)
{
    "inbounds": [
        {
            "port": 7890,
            "protocol": "http",
            "settings": {
                "auth": "noauth",
                "udp": true
            }
        }
    ],
    "outbounds": [
        {
            "protocol": "vmess",
            "settings": {
                "vnext": [
                    {
                        "address": "node-1504.yh82l3dsytxzf8.com",
                        "port": 30030,
                        "users": [
                            {
                                "id": "6e812e27-24df-37a5-98b2-adbba6633c34",
                                "alterId": 0,
                                "security": "auto"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "protocol": "freedom",
            "settings": {},
            "tag": "direct"
        }
    ],
    "routing": {
        "domainStrategy": "AsIs",
        "rules": [
            {
                "type": "field",
                "ip": [
                    "geoip:private"
                ],
                "outboundTag": "direct"
            }
        ]
    }
}

22、关于huggingface的镜像
    1）可以在.bashrc中设置：export HF_ENDPOINT=https://hf-mirror.com
    2）git clone https://hf-mirror.com/username/model_name
    3）以xinference为例，可以这样用：
        docker run --name Xinference \
          -v /mnt/user/appdata/Xinference/.xinference:/root/.xinference \
          -v /mnt/user/appdata/Xinference/.cache/huggingface:/root/.cache/huggingface \
          -v /mnt/user/appdata/Xinference/.cache/modelscope:/root/.cache/modelscope \
         --env "HF_ENDPOINT=https://hf-mirror.com" \
          -p 9997:9997 \
          --gpus all \
          xprobe/xinference \
          xinference-local -H 0.0.0.0

23、安装open-webui
    1)用下面命令启动
        sudo docker run -d -p 7864:8080 -e OPENAI_API_BASE_URLS="http://116.62.63.204:8001/v1" -e OPENAI_API_KEYS="empty" -v /home/tutu/open-webui-volume:/app/backend/data --name open-webui --restart no ghcr.io/open-webui/open-webui:main
        docker容器的前台启动调试：
            sudo docker run --rm -p 7864:8080 -e HF_ENDPOINT=https://hf-mirror.com -e OPENAI_API_BASE_URLS="http://116.62.63.204:8001/v1" -e OPENAI_API_KEYS="empty" -v /home/tutu/open-webui-volume:/app/backend/data --name open-webui --restart no ghcr.io/open-webui/open-webui:main
    2）需要启动一段时间，能通过 http://116.62.63.204:7864进入界面
    3）登录后，直接选择chat页面上方的model，选择qwen72，就可以用了

24、安装sillytavern(需要node.js, 不需要conda)
    git clone https://github.com/SillyTavern/SillyTavern -b release
    cd SillyTavern
    ./start.sh
    登录: 116.62.63.204:8000
    插头设置：api改为 Chat Completion，聊天补全来源改为 Custom(openai-compatible), Custom Endpoint改为 http://116.62.63.204:8001/v1，model ID和models改为qwen72。点击“连接”ok即可
    最右侧：选择角色，即可聊天

25、安装sd webui(需要已经开了梯子)
    cd ~
    sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
    wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
    chmod +x webui.sh
    ./webui.sh (安装在~/stable-diffusion-webui/下)
    下载模型到 /home/tutu/stable-diffusion-webui/models/Stable-diffusion
    报错：
        safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer
        解决：重新下载模型即可
    报错：
        Failed to create model quickly; will retry using slow method.
        解决：等待下载一个1.7G左右的模型
    到此可以运行绘图
    安装xformers：
        cd stable-diffusion-webui
        source venv/bin/activate
        pip install xformers -U
        pip install torch torchvision torchaudio -U （运行这个是为了把torch改为2.2.2，不改的话xformers有兼容问题，adetailer等插件会有问题）
        # 最早的pip install xformers最后会报错    (这里注意，如果pip报错SSLError,这种，往往是export https_proxy="http://127.0.0.1:7890"造成的，重新开一个控制台即可)
        deactivate
    运行:
        ~/webui.sh --xformers --no-half-vae --server 0.0.0.0 --port 7868

25-1、安装Stable Diffusion WebUI Docker（包含AUTOMATIC1111、InvokeAI和ComfyUI）（下载还是有问题，代理设置似乎都没用）
    git clone https://github.com/AbdBarho/stable-diffusion-webui-docker
    cd stable-diffusion-webui-docker
    下载：
        docker-compose --profile download up --build
    运行：
        docker-compose --profile auto up --build   (其中auto可以为：invoke、auto或comfy)

26、安装dify
    git clone https://github.com/langgenius/dify
    cd dify/docker
    修改docker-compose.yaml
        1）最下面nginx的端口为7866:80
        2）向量库改为: milvus
    docker compose up -d
    使用:
        1）添加本地自定义模型：右上角用户的设置里，模型供应商里增加OpenAI-API-compatible
        2）找一个模板，里面的gpt节点，点击一下，右侧节点说明出来后，点一下“模型”下拉框，然后再点一下里面的gpt模型图片，就能增加、换一个qwen节点；然后注意每一个节点的输入输出变量对不对，即可运行。

27、安装GPT-SoVITS
    conda create -n sovit python=3.9
    conda activate sovit
    git clone https://github.com/RVC-Boss/GPT-SoVITS
    cd GPT-SoVITS
    bash install.sh

    下载模型放到GPT_SoVITS/pretrained_models
        cd ~/GPT-SoVITS/GPT_SoVITS/pretrained_models
        git clone https://huggingface.co/lj1995/GPT-SoVITS
    下载UVR5（人声/伴奏分离及混响移除）放到tools/uvr5/uvr5_weights：
        下载https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/uvr5_weights中的所有文件
        上传到~/GPT-SoVITS/tools/uvr5/uvr5_weights
    下载中文ASR放到tools/asr/models：
        cd ~/GPT-SoVITS/tools/asr/models
        git clone https://www.modelscope.cn/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git
        git clone https://www.modelscope.cn/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch.git
        git clone https://www.modelscope.cn/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git
    运行：
        config.py中修改webui_port_main = 7867
        python webui.py (然后就能进入页面)
        仔细看说明：https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e
        参考项目：https://github.com/X-T-E-R/GPT-SoVITS-Inference



28、swift微调
    conda create -n swift python=3.10
    conda activate swift
    pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121
    pip install deepspeed==0.12.6
    git clone https://github.com/modelscope/swift.git
    cd swift
    pip install -v -e .[llm]
    vi sft.sh
#!/bin/bash
# Experimental environment: 2080ti 22G * 8
nproc_per_node=8
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
NPROC_PER_NODE=$nproc_per_node \
MASTER_PORT=29500 \
OMP_NUM_THREADS=1 \
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
swift sft \
    --model_type qwen1half-32b-chat  \
    --model_id_or_path /home/tutu/models/Qwen1.5-32B-Chat  \
    --sft_type lora \
    --tuner_backend swift \
    --template_type qwen \
    --dtype AUTO \
    --output_dir /home/tutu/swift/output \
    --ddp_backend nccl \
    --train_dataset_sample 5000 \
    --num_train_epochs 2 \
    --max_length 2048 \
    --check_dataset_strategy warning \
    --lora_rank 8 \
    --lora_alpha 32 \
    --lora_dropout_p 0.05 \
    --lora_target_modules DEFAULT \
    --gradient_checkpointing true \
    --batch_size 1 \
    --weight_decay 0.1 \
    --learning_rate 1e-4 \
    --gradient_accumulation_steps $(expr 16 / $nproc_per_node) \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.03 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 2 \
    --logging_steps 10 \
    --use_flash_attn false \
    --deepspeed default-zero3 \
    --save_only_model true \
    --save_safetensors false \
    --self_cognition_sample 1500 \
    --model_name 雷神 terrel \
    --model_author 土土 tutu \
    --push_to_hub false \
    --push_hub_strategy push_last  \

    vi merge.sh (微调完成后，lora和checkpoint的merge。注意：ckpt_dir千万不要搞错，如果搞错，swift并不会报错，导致merge出来的模型就是原模型)
CUDA_VISIBLE_DEVICES=0
swift export \
    --model_type qwen1half-32b-chat  \
    --model_id_or_path /home/tutu/models/Qwen1.5-32B-Chat  \
    --ckpt_dir '/home/tutu/swift/output/qwen1half-32b-chat/v1-20240410-103438/checkpoint-188' \
    --merge_lora true

29、超微4029服务器修改机箱风扇转速
    1)关于ipmitool权限(为tutu提供root权限，从而避免sudo ipmitool需要输入密码)：
        a)sudo apt install ipmitool
        b)编辑sudoers文件
            sudo visudo
            增加一行内容（增加后，Ctrl+x，然后y保存退出）：
                root    ALL=(ALL:ALL) ALL   （通常在这一行下面增加，注意visudo是直接输入，不是vi的那种方式）
                tutu ALL=(ALL) NOPASSWD: /usr/bin/ipmitool  (注意：经过测试，这行必须放在文件最后而不是上面这行之后，否则会被后面的组规则覆盖导致无效)
                也可以让全部命令sudo不需要密码：tutu ALL=(ALL) NOPASSWD: ALL （参数1为用户名，2为所在主机，3为代替的用户，4为ALL或NOPASSWD等命令，5为具体文件或ALL）
        c)sudo ipmitool（就不需要密码了）
    2)conda create -n ipmi python=3.10
    3)conda activate ipmi
    4)pip install streamlit
    5)pip install torch torchvision torchaudio (主要用于获取GPU信息)
    6)streamlit run ipmi.py --server.port 7860

    sudo ipmitool raw 0x30 0x70 0x66 0x01 0x00 0x04
    sudo ipmitool raw 0x30 0x70 0x66 0x01 0x01 0x04
    sudo ipmitool raw 0x30 0x70 0x66 0x01 0x02 0x08
    sudo ipmitool raw 0x30 0x70 0x66 0x01 0x03 0x08

29、超微4029服务器修改机箱风扇转速（最终方案）（前述方案，optimal方式下风扇仍会时不时提高转速）
    1）fan模式修改为full
    2）重启后，输入一次以下命令即可
        sudo ipmitool raw 0x30 0x70 0x66 0x01 0x00 0x04
        sudo ipmitool raw 0x30 0x70 0x66 0x01 0x01 0x04
        sudo ipmitool raw 0x30 0x70 0x66 0x01 0x02 0x08
        sudo ipmitool raw 0x30 0x70 0x66 0x01 0x03 0x08

29-1、ipmi自动启动(注意这里要确保执行sudo已经不需要密码，详见上一条)
    sudo vi /etc/systemd/system/ipmilowfan.service(注意第一行必须要有，不然exec报错)
#!/bin/bash
sudo ipmitool raw 0x30 0x70 0x66 0x01 0x00 0x04
sudo ipmitool raw 0x30 0x70 0x66 0x01 0x01 0x04
sudo ipmitool raw 0x30 0x70 0x66 0x01 0x02 0x08
sudo ipmitool raw 0x30 0x70 0x66 0x01 0x03 0x08

[Unit]
Description=ipmilowfan
After=network.target
[Service]
Type=simple
User=tutu
ExecStart=/home/tutu/ipmilowfan.sh
ExecReload=/home/tutu/ipmilowfan.sh
ExecStop=/usr/bin/pkill /home/tutu/ipmilowfan.sh
KillMode=process
Restart=on-failure
RestartSec=15s
[Install]
WantedBy=multi-user.target

        sudo systemctl daemon-reload
        sudo systemctl start ipmilowfan.service
        sudo systemctl status ipmilowfan.service
        sudo systemctl enable ipmilowfan.service

30、关于streamlit报错
    现象：OSError: [Errno 28] inotify watch limit reached
    解决：（似乎就是不能在~/下运行）把ipmi.py文件复制到/home/tutu/server/life-agent下运行即可：streamlit run ipmi.py --server.port 7860

31、Comfy UI安装
    conda create -n comfy python=3.11
    conda activate comfy
    pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121
    git clone https://github.com/comfyanonymous/ComfyUI
    cd ComfyUI
    pip install -r requirements.txt
    python main.py --listen=0.0.0.0 --port=7869
    模型复制到：models/checkpoints
        cd /home/tutu/ComfyUI/models/checkpoints
        ln -s /home/tutu/stable-diffusion-webui/models/Stable-diffusion/* ./
    VAE复制到：models/vae
    (注意：open webui中，图像设置中设置http://116.62.63.204:7869以及模型参数后，聊天界面并不是agent方式绘图，而是对话输出后通过点击绘图按钮绘制和对话内容对应的图片)

    1）impact pack安装（脸部修复）
        a）安装manager
            cd ComfyUI/custom_nodes
            git clone https://github.com/ltdrdata/ComfyUI-Manager.git
            重启comfyui
        b）会发现UltralyticsDetectorProvider节点添加时界面卡死，需要做如下工作：
            1）/home/tutu/ComfyUI/models/ultralytics/bbox下
            把https://huggingface.co/Bingsu/adetailer/tree/main下的：
                face_yolov8n.pt
                face_yolov8s.pt
                hand_yolov8n.pt
                person_yolov8n-seg.pt
                person_yolov8s-seg.pt
            等文件复制到/home/tutu/ComfyUI/models/ultralytics/bbox
            2）/home/tutu/ComfyUI/models/sams下
                wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth
        c)脸部修复的连接：
            0）FaceDetailer的model、clip、vae、positive、negative从ksampler挪过来，ksampler删除即可
            1）UltralyticsDetectorProvider   -> FaceDetailer的bbox_detector
            2）SAMLoader                     -> FaceDetailer的sam_model_opt
            3）pic                           -> FaceDetailer的输入image
            4）FaceDetailer的输出image -> save
    2）关于api导出
        1）flow中最终用于api获取的出图节点，由普通image节点改为'Add Node->api->image->SaveImageWebsocket'节点
        2）打开Queue Prompt按钮右上角的设置按钮，打开开发者模式
        3）此时出现Save（Api Format）按钮，导出json文件就可以用python直接调用。可以修改json中的参数。

32、FastLLM安装和运行deepseek 236b-int4-flm
    1）conda create -n fastllm python=3.11, conda activate fastllm
    2)git clone https://github.com/ztxz16/fastllm, cd fastllm
    3)mkdir build, cd build
    4)编译gpu版
        安装cmake：sudo apt install cmake
        修改配置：CMakeLists.txt文件中，set(CMAKE_CUDA_ARCHITECTURES "native")的"native"改为gpu对应能力数值（2080ti是75）
        编译：
            cmake .. -DUSE_CUDA=ON
            make -j
            cd tools
            python setup.py install
    5）关于下载的deepseek 236b-int4-flm模型文件夹，要在文件夹里，进行文件连接，然后运行是调用连接后的完整flm文件，不然报错：
        cat deepseekv2-chat-int4g256.flm.part_* > deepseekv2-chat-int4g256.flm
    6）调用GPU运行模型

33、安装和运行redis（端口为8010）
    1）安装redis服务
        sudo apt install redis-server
        sudo systemctl start redis-server
        sudo systemctl enable redis-server
        sudo systemctl status redis-server
    2）配置redis
        sudo vi /etc/redis/redis.conf
        bind 127.0.0.1 改为 bind 0.0.0.0
        port 6379 改为 8010
        sudo systemctl restart redis-server
        验证：
            redis-cli -p 8010
            ping（如果返回PONG则正常）
    3）win下安装redis
        https://github.com/microsoftarchive/redis/releases下载最新msi，安装即可
        控制台下验证：
            redis-cli -h 116.62.63.204 -p 8010
            ping（如果返回PONG则正常）
    4）安装redis insight
        下载：https://redis.io/insight/
        添加database：116.62.63.204 8010
        验证：点击所添加的database后，在界面左下角">_CLI"中输入ping（如果返回PONG则正常）
    5）安装redis-py(发布message，每一秒+1)
        选择conda环境，如：conda activate client
        pip install redis (successfully installed redis-5.0.4)
        python redis_a.py
            import redis
            import time
            # 连接到 Redis 服务器
            r = redis.Redis(host='localhost', port=8010, db=0)

            message = 0
            while True:
                message += 1
                r.set('message', message)
                print(f"Published: {message}")
                time.sleep(1)

        python redis_b.py(每0.5秒读取message)
            import redis
            import time
            # 连接到 Redis 服务器
            r = redis.Redis(host='localhost', port=8010, db=0)

            while True:
                message = r.get('message')
                if message:
                    print(f"Received: {int(message)}")
                time.sleep(0.5)

34、MediaCrawler(目前仅限于有图形界面的系统，因为需要扫码)
    1)安装
        conda create -n craw python=3.11, conda activate craw
        git clone https://github.com/NanmiCoder/MediaCrawler
        cd MediaCrawler
        pip install -r requirements.txt
        playwright install
    2）运行（配置文件为：config/base_config.py）
        ### 项目默认是没有开启评论爬取模式，如需评论请在config/base_config.py中的 ENABLE_GET_COMMENTS 变量修改
        ### 一些其他支持项，也可以在config/base_config.py查看功能，写的有中文注释

        # 从配置文件中读取关键词搜索相关的帖子并爬取帖子信息与评论
        python main.py --platform xhs --lt qrcode --type search

        # 从配置文件中读取指定的帖子ID列表获取指定帖子的信息与评论信息
        python main.py --platform xhs --lt qrcode --type detail

        # 打开对应APP扫二维码登录

        # 其他平台爬虫使用示例，执行下面的命令查看
        python main.py --help
    3）数据保存位置
        支持保存到关系型数据库（Mysql、PgSQL等）
        执行 python db.py 初始化数据库数据库表结构（只在首次执行）
        支持保存到csv中（data/目录下）
        支持保存到json中（data/目录下）

35、Scrapy
    conda create -n craw python=3.11
    conda activate craw
    pip install Scrapy

36、win下安装ChatTTS
    安装ChatTTS
        git clone https://github.com/2noise/ChatTTS
        conda create -n tts python=3.11
        conda activate tts
        cd ChatTTS
        pip install -r requirements.txt
    新版本windows上报错 Normalizer pynini WeTextProcessing nemo_text_processing 解决方法
        打开 ChatTTS/core.py, 大约143行，注释掉接下来的7行（if do_text_normalization:等7行）即可
    安装模型
        git clone https://huggingface.co/2Noise/ChatTTS
    python webui.py --local_path=d:/models/ChatTTS

37、安装最新版本yarn
    sudo apt update
    sudo apt install curl
    curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -(需要开vpn：~/proxy_git.sh)
    echo "deb https://dl.yarnpkg.com/debian/ stable main" | sudo tee /etc/apt/sources.list.d/yarn.list
    sudo apt install yarn
    yarn --version(此时为0.32+git)
    sudo apt install npm
    sudo curl --compressed -o- -L https://yarnpkg.com/install.sh | bash
    重启终端后：yarn --version(此时为1.22.22)

38、安装llm-viz
    git clone https://github.com/bbycroft/llm-viz
    cd llm-viz
    yarn
    修改package.json中"scripts"的"dev"的ip和port:
        "dev": "next dev -H 0.0.0.0 -p 7869",
    yarn dev

38、用autogptq量化模型
    安装autogptq：
        conda create -n autogptq python=3.11
        conda activate autogptq
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
        pip install auto-gptq --no-build-isolation
    量化：
        pip install protobuf
        运行下面的python代码，gptq文件夹出现下列文件：
            config.json
            quantize_config.json
            gptq_model-4bit-128g.safetensors
        然后将原模型文件夹中的下列文件，copy到gptq文件夹中：
            special_tokens_map.json
            tokenizer_config.json
            tokenizer.json
            tokenizer.model
        运行vllm即可

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

pretrained_model_dir = "/home/tutu/models/openbuddy-mistral-22b-v21.1-32k"
quantized_model_dir = "/home/tutu/models/openbuddy-mistral-22b-v21.1-32k-gptq"

tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)
examples = [
    tokenizer(
        "auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."
    )
]

quantize_config = BaseQuantizeConfig(
    bits=4,  # quantize model to 4-bit
    group_size=128,  # it is recommended to set the value to 128
    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad
)

# load un-quantized model, by default, the model will always be loaded into CPU memory
model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)

# quantize model, the examples should be list of dict whose keys can only be "input_ids" and "attention_mask"
model.quantize(examples)

# save quantized model
model.save_quantized(quantized_model_dir)

# save quantized model using safetensors
model.save_quantized(quantized_model_dir, use_safetensors=True)
print('save_quantized() finished.')

39、llama.cpp跑deepseek-coder-v2-instruct gguf（llama.cpp在cpu上可以跑，gpu上跑出错）
    conda create -n gguf python=3.11
    conda activate gguf
    git clone https://github.com/ggerganov/llama.cpp
    cd llama.cpp
    LLAMA_CUDA=1 LLAMA_CUDA_DMMV_F16=1 make

    export LLAMA_CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    ./llama-server -m ~/models/DeepSeek-Coder-V2-Instruct-GGUF/DeepSeek-Coder-V2-Instruct.Q4_K_M.gguf --port 8001 --host 0.0.0.0 --threads 16 --gpu-layers 50 --batch-size 16 --ctx-size 4096

    【ollama方案（可行）】
        【最新发现】参考：https://ollama.com/library/deepseek-coder-v2:236b-instruct-q4_k_m
            安装：curl -fsSL https://ollama.com/install.sh | sh
            运行：
                ollama serve
                ollama run deepseek-coder-v2:236b-instruct-q4_k_m

        【老的方式】ollama的Modelfile参数见：https://okhand.org/post/ollama-modefile/
        安装：curl -fsSL https://ollama.com/install.sh | sh
        vi Modelfile:
            From /home/tutu/models/DeepSeek-Coder-V2-Instruct-GGUF/DeepSeek-Coder-V2-Instruct.Q4_K_M.gguf
        ollama create ds -f ./Modelfile （133G模型transfer到.ollama/下大概需要270G的临时空间）
        export OLLAMA_HOST=0.0.0.0:8001
        export OLLAMA_KEEPALIVE=24h （不设置会在5分钟自动关掉serve）
        export OLLAMA_NUM_PARALLEL=4
        ollama run ds

        要关掉serve目前必须kill，后续行为比较奇怪：
            再启动需要运行ollama serve
            然后另一个控制台，export变量，然后ollama run ds
            如果删除了/tmp/ollama*，可能需要重新ollama create ds -f ./Modelfile

        如果ollama run deepseek-coder-v2:236b-instruct-q4_k_m等命令报错：Error: [0] server cpu not listed in available servers map[]
            sudo service ollama restart，即可

            # --ctx-size 2048 --batch-size 512 --embedding --log-disable --n-gpu-layers 61 --parallel 1 --tensor-split 8,8,8,8,8,7
            # /tmp/ollama322037977/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-ece89e24aa9899d30457e4f1e089286b18c29af6f3d528e1a8597a7bbcb1929d --ctx-size 2048 --batch-size 512 --embedding --log-disable --n-gpu-layers 61 --parallel 1 --tensor-split 8,8,8,8,8,7


