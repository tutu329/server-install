一、ubuntu安装
1、必须要有个u盘，如果只是把准备装系统的移动硬盘分一个区用来放ubuntu镜像，则会在ubuntu安装时需要重新分区移动硬盘而导致安装失败（移动硬盘包含安装数据）
2、分区配置：
	a）efi分区：逻辑分区、efi、2.5g
	b）swap分区：逻辑分区、64G
	c）/：主分区、ext4、若干
	d）/home：逻辑分区、ext4、若干
3、n卡驱动安装：
1）千万不要参照网上的流程（关nouveau之类），会导致重启后系统黑屏无法用、必须重装，而是直接在图形界面的“附加驱动”中用如第一行的较新nvidia驱动，重启即可
2）nvidia-smi测试是否成功。nvlink可以通过nvidia-smi topo -m测试
4、根据要求安装cuda-toolkit，如需要cuda12.1,则按照https://developer.nvidia.com/cuda-12-1-1-download-archive的要求安装即可（主要用于vllm的编译）
5、安装anaconda，安装时注意要让其执行conda init，不然有些问题（如conda找不到、conda activate报错等，安装后执行也可以）
6、在conda环境中安装pytorch2.1.1+cuda12.1: pip install torch torchvision torchaudio

二、安装gcc
sudo apt update
sudo apt install g++
sudo apt install gcc
sudo apt install make

三、安装llm
1、conda create -n vllm python=3.10
2、conda activate vllm
3、git clone https://github.com/vllm-project/vllm.git
4、vllm文件夹里：pip install -e .  # This may take 5-10 minutes.
5、运行vllm应用主要问题：
	a)vllm版本有些乱，from vllm import LLM可能要改为from vllm.entrypoint... import ...这类，具体可以翻源码结构就行
	b)参数如多gpu：
		from vllm.engine.arg_utils import AsyncEngineArgs
	    	engine_args = AsyncEngineArgs(model_path)
	    	engine_args.trust_remote_code = True
	    	engine_args.tensor_parallel_size = 2（即使用2个gpu）
	c)之后运行vllm应用，可能会报错(Failed to start the dashboard)，可以pip install -r requirement.txt即可
	d)运行server(openai)： 
		python -m vllm.entrypoints.openai.api_server --model ~/models/Qwen-14B-Chat --tensor-parallel-size 2 --trust-remote-code
		http://localhost:8001/docs查看api
	e)fastchat的安装：
		按照官网要求：pip install "fschat[model_worker,webui]"
	f)运行client.py:
		"""Example Python client for vllm.entrypoints.api_server"""

		import argparse
		import json
		from typing import Iterable, List

		import requests


		def clear_line(n: int = 1) -> None:
		    LINE_UP = '\033[1A'
		    LINE_CLEAR = '\x1b[2K'
		    for _ in range(n):
			print(LINE_UP, end=LINE_CLEAR, flush=True)


		def post_http_request(prompt: str,
				      api_url: str,
				      n: int = 1,
				      stream: bool = False) -> requests.Response:
		    headers = {"User-Agent": "Test Client"}
		    pload = {
			"prompt": prompt,
			"n": n,
			"use_beam_search": True,
			"temperature": 0.0,
			"max_tokens": 32,
			"stream": stream,
		    }
		    response = requests.post(api_url, headers=headers, json=pload, stream=True)
		    return response


		def get_streaming_response(response: requests.Response) -> Iterable[List[str]]:
		    for chunk in response.iter_lines(chunk_size=8192,
				                     decode_unicode=False,
				                     delimiter=b"\0"):
			if chunk:
			    data = json.loads(chunk.decode("utf-8"))
			    output = data["text"]
			    yield output


		def get_response(response: requests.Response) -> List[str]:
		    data = json.loads(response.content)
		    output = data["text"]
		    return output


		if __name__ == "__main__":
		    parser = argparse.ArgumentParser()
		    parser.add_argument("--host", type=str, default="localhost")
		    parser.add_argument("--port", type=int, default=8001)
		    parser.add_argument("--n", type=int, default=4)
		    parser.add_argument("--prompt", type=str, default="西湖是杭州的明珠，")
		    parser.add_argument("--stream", action="store_true")
		    args = parser.parse_args()
		    prompt = args.prompt
		    api_url = f"http://{args.host}:{args.port}/generate"
		    n = args.n
		    # stream = args.stream
		    stream=True

		    print(f"Prompt: {prompt!r}\n", flush=True)
		    response = post_http_request(prompt, api_url, n, stream)

		    if stream:
			num_printed_lines = 0
			for h in get_streaming_response(response):
			    clear_line(num_printed_lines)
			    num_printed_lines = 0
			    for i, line in enumerate(h):
				num_printed_lines += 1
				print(f"Beam candidate {i}: {line!r}", flush=True)
		    else:
			output = get_response(response)
			for i, line in enumerate(output):
			    print(f"Beam candidate {i}: {line!r}", flush=True)
