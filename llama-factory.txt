1、llama-factory安装
    conda create -n llamafactory python=3.11
    conda activate llamafactory
    git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
    cd LLaMA-Factory
    pip install -e .[all]
    pip install deepspeed

2、界面
    启动界面：
        GRADIO_SERVER_PORT=7860 llamafactory-cli webui
        (注意：需要在llama-factory所在目录如/home/tutu/LLaMA-Factory下运行，否则会导致data文件夹找不到等问题)


3、流程
    1）cpt(Continue Pre-Train)
        界面参数：
            模型名称：Qwen2.5-1.5B
            模型路径：/home/tutu/models/Qwen2.5-1.5B
            微调方法：full
            训练阶段：Pre-Training(默认是Supervised Fine-Tuning，训练后输出是增量)
            数据集：c4_demo
            计算类型：fp16（默认是bf16，2080ti会报错）
            DeepSpeed stage：3（默认是none，会报oom）
            批处理大小和截断长度（建议1和4096）：4和1024（默认是2，如果改为8及以上，就会oom），或者2和2048，或者1和4096、1和8192（16384会oom，32000会报数据不够长的错误）
            输出目录：如train_2024-11-05-14-46-20_cpt
            最大样本数：10000000000（这个很关键，否则jsonl文件再多，也只训练一部分。如5GB多的jsonl文件，样本数能达到597943079）
            配置路径：2024-11-05-14-56-41.yaml
            其他参数设置->保存间隔：5000
            其他参数设置->额外参数（save_only_model设置为true，保存间隔设置为5000，确保预训练1.5B模型时每5000个step只生成一个3GB的checkpoint文件夹。否则预训练1.5B模型、90GB数据，70000steps，每100个step就会生成一个21GB的中间checkpoint文件夹（save_steps默认为100））：
                {"optim": "adamw_torch", "save_only_model":true}
        然后可以点“开始”，该demo数据集，训练几秒钟后即完成，模型会输出到/home/tutu/LLaMA-Factory/saves/Qwen2.5-1.5B/full下。报错可看后台信息。
        注意：92GB的jsonl文件，数据预处理大概要个把小时，但是后续再用于训练时，就不需要再预处理，会在~/.cache/huggingface/datasets下保留约278GB的缓存。
    2）sft(Supervised Fine-Tuning)
        界面参数：
            微调方法：full
            训练阶段：Supervised Fine-Tuning
            检查点路径：如前述cpt输出的“train_2024-11-05-14-46-20_cpt” （注意：该参数如果为空，则可以通过“预览命令”看到sft的是原版模型而非cpt输出模型，即--model_name_or_path /home/tutu/models/Qwen2.5-1.5B）
            数据集：alpaca_zh_demo
            批处理大小和截断长度（建议8和8192）：8和2048（默认是2，如果改为16及以上，就会oom），或8和4096，或8和128000都行（可能demo数据实际没有很长的QA）
            输出目录：如train_2024-11-05-14-46-20_sft（要和上面cpt输出的base模型区分）
            配置路径：2024-11-05-14-56-41_sft.yaml
        然后可以点“开始”，该demo数据集，训练几秒钟后即完成，模型会输出到/home/tutu/LLaMA-Factory/saves/Qwen2.5-1.5B/full下。报错可看后台信息。
    3、chat测试
        检查点路径：设置为“train_2024-11-05-14-46-20_cpt”或“train_2024-11-05-14-46-20_sft”
        “Chat”中“加载模型”，然后对话即可，cpt为base模型，sft为对话模型。
    *、导出
        cpt或sft完成后，“检查点路径”选择为训练输出的模型如“train_2024-11-05-10-52-37_cpt”或“train_2024-11-05-10-52-37_sft”，然后“export”中，“导出目录”填写如/home/tutu/my_models/xxx，点“开始导出”，即可。
        然后即可用vllm推理/home/tutu/my_models/xxx的模型

4、其他
    1）关于Cutoff length
        a）参考：https://github.com/hiyouga/LLaMA-Factory/issues/5024
            I don't quite understand what "suppose better than cutoff_length=2048" is.
            Actually, I'm a beginner, but I think it depends on what you're trying to do,
            if you want longer context, cutoff_length=12000 is better, for the question you're referencing,
                pt采用文本分段，而不是根据cutoff_length进行truncate，【if it's pre-training, it automatically segments for you,it doesn't truncate, 】
                sft根据cutoff_length进行truncate，【if it's SFT it truncates.】
        b）参考：https://github.com/hiyouga/LLaMA-Factory/issues/4657
            Q：I'm using llama-factory to do continual pre-training,
                and my data is a jsonl file with each line's content >10k tokens.
                I'm wondering if i set the cutoff_len to 1024,
                then most of the information will be cutoff?
                Should i do chunking manually before running the training script or is there some easy way to do this?
            A：See this section for the packing operation we used in the pre-training stage:
                https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt#preparing-the-dataset
        结论：pretrain时，训练集的长文，是被分段后进行训练，不涉及到cutoff_length的截断操作，最多只在长文最后一段（该段不够长时）才可能涉及到drop。

    2）关于训练刚开始时的overflow（参考：https://github.com/THUDM/CogVideo/issues/233）
        报错：
            [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
        建议：
            It is normal to skip when the loss is large at the beginning of training.
            You can find that a small number of steps will be skipped in the first 50 steps.
            Once the training is stable, it will not happen again.
    3）报错：Token indices sequence length is longer than the specified maximum sequence length for this model (159603 > 131072). Running this sequence through the model will result in indexing errors
        结论：根据issue中hiyouga反馈，不影响训练，也和cutoff length无关。（https://github.com/hiyouga/LLaMA-Factory/issues/5952）

5、数据相关
    1）parquet列式数据格式
        a）win下的parquet显示工具（https://github.com/mukunku/ParquetViewer/releases）
        b）parquet转jsonl
            见server/life-agent/tools/data_tool/parquet.py
    2)llama-factory中的dataset_info.json
        a)自定义的pt数据的引用要采用下面格式，可参考自带的c4_demo.json（要注意，否则llama-factory会默认是sft数据）
          "00": {
            "file_name": "/home/tutu/data/train-00000-of-00192.jsonl",
            "columns": {
              "prompt": "text"
            }
          },
          "01": {
            "file_name": "/home/tutu/data", （这样就可以训练data下的所有如jsonl文件，但是data下必须只有jsonl一种类型的文件，而且不能有其他文件夹包括.xxx这种文件夹，否则会报错）
            "columns": {
              "prompt": "text"
            }
          },

6、tensorboard
    1）将gpu_server/train_tools下的tensorboard.sh和trainer_log_to_tensorboard.py复制到如/home/tutu/LLaMA-Factory/saves/Qwen2.5-1.5B/full/cpt_all_2024-11-11-09-41-24下
    2）pip install tensorboard
    3）./tensorboard.sh
    4）打开网页，展开loss页面

附）2024-11-27训练情况：
    模型基座：/home/tutu/models/Qwen2.5-1.5B
    训练时间：16天3小时
    epoch：1
    loss：3.3->2.4左右
    steps：75810
    训练平均速度：18.4s/it

    训练参数：
    cutoff length	4096
    epoches		1
    max samples	10000000000
    learning rate	5e-5
    batch size		1
    deepspeed stage	3
    compute type	fp16
    saving steps	5000
    extra arguments	{"optim": "adamw_torch", "save_only_model":true}

    datasets		"all"
    数据集配置文件：/home/tutu/LLaMA-Factory/data/dataset_info.json
    {
      "all": {
        "file_name": "/home/tutu/data/xxx/jsonl",
        "columns": {
          "prompt": "text"
        }
      }
    }

7、使用neo-map处理数据(MAP-NEO/Matrix/document-convert)
    0）下载
        git clone https://github.com/multimodal-art-projection/MAP-NEO
        cd MAP-NEO/Matrix/document-convert
    1）安装
        conda create -n neo python=3.9(FastDeploy之类必须用3.9)
        conda activate neo
        pip install -r requirements.txt
        python -m pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html
    2）可能的报错
        报错：ImportError: libcudart.so.11.0: cannot open shared object file: No such file or directory
        (尚未解决)解决：可能需要在ubuntu安装多版本cuda toolkit(https://blog.csdn.net/weixin_64174049/article/details/140065229?spm=1001.2014.3001.5506)

        报错：cannot import name 'xyxy24p' from 'cnstd.yolov7.general'
        解决：pip install cnstd==1.2.3