1、llama-factory安装
    conda create -n llamafactory python=3.11
    conda activate llamafactory
    git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
    cd LLaMA-Factory
    pip install -e .[all]
    pip install deepspeed

2、界面
    启动界面：
        GRADIO_SERVER_PORT=7860 llamafactory-cli webui


3、流程
    1）cpt(Continue Pre-Train)
        界面参数：
            模型名称：Qwen2.5-1.5B
            模型路径：/home/tutu/models/Qwen2.5-1.5B
            微调方法：full
            训练阶段：Pre-Training(默认是Supervised Fine-Tuning，训练后输出是增量)
            数据集：c4_demo
            计算类型：fp16（默认是bf16，2080ti会报错）
            DeepSpeed stage：3（默认是none，会报oom）
            批处理大小和截断长度（建议1和8192）：4和1024（默认是2，如果改为8及以上，就会oom），或者2和2048，或者1和4096、1和8192（16384会oom，32000会报数据不够长的错误）
            输出目录：如train_2024-11-05-14-46-20_cpt
            配置路径：2024-11-05-14-56-41.yaml
        然后可以点“开始”，该demo数据集，训练几秒钟后即完成，模型会输出到/home/tutu/LLaMA-Factory/saves/Qwen2.5-1.5B/full下。报错可看后台信息。
    2）sft(Supervised Fine-Tuning)
        界面参数：
            微调方法：full
            训练阶段：Supervised Fine-Tuning
            检查点路径：如前述cpt输出的“train_2024-11-05-14-46-20_cpt” （注意：该参数如果为空，则可以通过“预览命令”看到sft的是原版模型而非cpt输出模型，即--model_name_or_path /home/tutu/models/Qwen2.5-1.5B）
            数据集：alpaca_zh_demo
            批处理大小和截断长度（建议8和8192）：8和2048（默认是2，如果改为16及以上，就会oom），或8和4096，或8和128000都行（可能demo数据实际没有很长的QA）
            输出目录：如train_2024-11-05-14-46-20_sft（要和上面cpt输出的base模型区分）
            配置路径：2024-11-05-14-56-41_sft.yaml
        然后可以点“开始”，该demo数据集，训练几秒钟后即完成，模型会输出到/home/tutu/LLaMA-Factory/saves/Qwen2.5-1.5B/full下。报错可看后台信息。
    3、chat测试
        检查点路径：设置为“train_2024-11-05-14-46-20_cpt”或“train_2024-11-05-14-46-20_sft”
        “Chat”中“加载模型”，然后对话即可，cpt为base模型，sft为对话模型。
    *、导出
        cpt或sft完成后，“检查点路径”选择为训练输出的模型如“train_2024-11-05-10-52-37_cpt”或“train_2024-11-05-10-52-37_sft”，然后“export”中，“导出目录”填写如/home/tutu/my_models/xxx，点“开始导出”，即可。
        然后即可用vllm推理/home/tutu/my_models/xxx的模型

4、其他
    1）关于Cutoff length
        a）参考：https://github.com/hiyouga/LLaMA-Factory/issues/5024
            I don't quite understand what "suppose better than cutoff_length=2048" is.
            Actually, I'm a beginner, but I think it depends on what you're trying to do,
            if you want longer context, cutoff_length=12000 is better, for the question you're referencing,
                pt采用文本分段，而不是根据cutoff_length进行truncate，【if it's pre-training, it automatically segments for you,it doesn't truncate, 】
                sft根据cutoff_length进行truncate，【if it's SFT it truncates.】
        b）参考：https://github.com/hiyouga/LLaMA-Factory/issues/4657
            Q：I'm using llama-factory to do continual pre-training,
                and my data is a jsonl file with each line's content >10k tokens.
                I'm wondering if i set the cutoff_len to 1024,
                then most of the information will be cutoff?
                Should i do chunking manually before running the training script or is there some easy way to do this?
            A：See this section for the packing operation we used in the pre-training stage:
                https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt#preparing-the-dataset
        结论：pretrain时，训练集的长文，是被分段后进行训练，不涉及到cutoff_length的截断操作，最多只在长文最后一段（该段不够长时）才可能涉及到drop。

    2）使用自定义数据集时，需更新 data/dataset_info.json 文件

5、数据相关
    1）parquet列式数据格式
        a）win下的parquet显示工具（https://github.com/mukunku/ParquetViewer/releases）
        b）parquet转jsonl
            见server/life-agent/tools/data_tool/parquet.py



