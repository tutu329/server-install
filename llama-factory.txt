1、llama-factory安装
    conda create -n llamafactory python=3.11
    conda activate llamafactory
    git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
    cd LLaMA-Factory
    pip install -e .[all]
    pip install deepspeed

2、微调example
    llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
    llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
    llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
    使用自定义数据集时，需更新 data/dataset_info.json 文件

3、界面
    启动界面：
        GRADIO_SERVER_PORT=7860 llamafactory-cli webui


4、流程
    1）cpt(Continue Pre-Train)
        界面参数：
            模型名称：Qwen2.5-1.5B
            模型路径：/home/tutu/models/Qwen2.5-1.5B
            微调方法：full
            训练阶段：Pre-Training(默认是Supervised Fine-Tuning，训练后输出是增量)
            数据集：c4_demo
            计算类型：fp16（默认是bf16，2080ti会报错）
            DeepSpeed stage：3（默认是none，会报oom）
            批处理大小：4（默认是2，如果改为8及以上，就会oom）
            输出目录：train_2024-11-05-14-46-20（会自动按当前时间设置）
            配置路径：2024-11-05-14-56-41.yaml
        然后可以点“开始”，该demo数据集，训练几秒钟后即完成，模型会输出到/home/tutu/LLaMA-Factory/saves/Qwen2.5-1.5B/full下。报错可看后台信息。
    2）sft(Supervised Fine-Tuning)
        界面参数：
            微调方法：full
            训练阶段：Supervised Fine-Tuning
            数据集：alpaca_zh_demo
            批处理大小：8（默认是2，如果改为16及以上，就会oom）
            输出目录：train_2024-11-05-14-46-20_sft（要和上面cpt输出的base模型区分）
            配置路径：2024-11-05-14-56-41_sft.yaml
        然后可以点“开始”，该demo数据集，训练几秒钟后即完成，模型会输出到/home/tutu/LLaMA-Factory/saves/Qwen2.5-1.5B/full下。报错可看后台信息。
    *、导出
        cpt或sft完成后，“检查点路径”选择为训练输出的模型如“train_2024-11-05-10-52-37”，然后“export”中，“导出目录”填写如/home/tutu/my_models/xxx，点“开始导出”，即可。
        然后即可用vllm推理/home/tutu/my_models/xxx的模型

