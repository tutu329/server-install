1、llama-factory安装
    conda create -n llamafactory python=3.11
    conda activate llamafactory
    git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
    cd LLaMA-Factory
    pip install -e .[all]
    pip install deepspeed（如果lf用了0.92dev版报错：no_sync context manager is incompatible，则pip install deepspeed==0.15.4）

2、界面
    启动界面：
        GRADIO_SERVER_PORT=7860 llamafactory-cli webui
        (注意：需要在llama-factory所在目录如/home/tutu/LLaMA-Factory下运行，否则会导致data文件夹找不到等问题)


3、流程
    1）cpt(Continue Pre-Train)
        界面参数：
            模型名称：Qwen2.5-1.5B
            模型路径：/home/tutu/models/Qwen2.5-1.5B
            微调方法：full
            训练阶段：Pre-Training(默认是Supervised Fine-Tuning，训练后输出是增量)
            数据集：c4_demo
            计算类型：fp16（默认是bf16，2080ti会报错）
            DeepSpeed stage：3（默认是none，会报oom）
            批处理大小和截断长度（建议1和4096）：4和1024（默认是2，如果改为8及以上，就会oom），或者2和2048，或者1和4096、1和8192（16384会oom，32000会报数据不够长的错误）
            输出目录：如train_2024-11-05-14-46-20_cpt
            最大样本数：10000000000（这个很关键，否则jsonl文件再多，也只训练一部分。如5GB多的jsonl文件，样本数能达到597943079）
            配置路径：2024-11-05-14-56-41.yaml
            其他参数设置->保存间隔：5000
            其他参数设置->额外参数（save_only_model设置为true，保存间隔设置为5000，确保预训练1.5B模型时每5000个step只生成一个3GB的checkpoint文件夹。否则预训练1.5B模型、90GB数据，70000steps，每100个step就会生成一个21GB的中间checkpoint文件夹（save_steps默认为100））：
                {"optim": "adamw_torch", "save_only_model":true}
        然后可以点“开始”，该demo数据集，训练几秒钟后即完成，模型会输出到/home/tutu/LLaMA-Factory/saves/Qwen2.5-1.5B/full下。报错可看后台信息。
        注意：92GB的jsonl文件，数据预处理大概要个把小时，但是后续再用于训练时，就不需要再预处理，会在~/.cache/huggingface/datasets下保留约278GB的缓存。
    2）sft(Supervised Fine-Tuning)
        界面参数：
            微调方法：full
            训练阶段：Supervised Fine-Tuning
            检查点路径：如前述cpt输出的“train_2024-11-05-14-46-20_cpt” （注意：该参数如果为空，则可以通过“预览命令”看到sft的是原版模型而非cpt输出模型，即--model_name_or_path /home/tutu/models/Qwen2.5-1.5B）
            数据集：alpaca_zh_demo
            批处理大小和截断长度（建议8和8192）：8和2048（默认是2，如果改为16及以上，就会oom），或8和4096，或8和128000都行（可能demo数据实际没有很长的QA）
            输出目录：如train_2024-11-05-14-46-20_sft（要和上面cpt输出的base模型区分）
            配置路径：2024-11-05-14-56-41_sft.yaml
        然后可以点“开始”，该demo数据集，训练几秒钟后即完成，模型会输出到/home/tutu/LLaMA-Factory/saves/Qwen2.5-1.5B/full下。报错可看后台信息。
    3、chat测试
        检查点路径：设置为“train_2024-11-05-14-46-20_cpt”或“train_2024-11-05-14-46-20_sft”
        “Chat”中“加载模型”，然后对话即可，cpt为base模型，sft为对话模型。
    *、导出
        cpt或sft完成后，“检查点路径”选择为训练输出的模型如“train_2024-11-05-10-52-37_cpt”或“train_2024-11-05-10-52-37_sft”，然后“export”中，“导出目录”填写如/home/tutu/my_models/xxx，点“开始导出”，即可。
        然后即可用vllm推理/home/tutu/my_models/xxx的模型

4、其他
    1）关于Cutoff length
        a）参考：https://github.com/hiyouga/LLaMA-Factory/issues/5024
            I don't quite understand what "suppose better than cutoff_length=2048" is.
            Actually, I'm a beginner, but I think it depends on what you're trying to do,
            if you want longer context, cutoff_length=12000 is better, for the question you're referencing,
                pt采用文本分段，而不是根据cutoff_length进行truncate，【if it's pre-training, it automatically segments for you,it doesn't truncate, 】
                sft根据cutoff_length进行truncate，【if it's SFT it truncates.】
        b）参考：https://github.com/hiyouga/LLaMA-Factory/issues/4657
            Q：I'm using llama-factory to do continual pre-training,
                and my data is a jsonl file with each line's content >10k tokens.
                I'm wondering if i set the cutoff_len to 1024,
                then most of the information will be cutoff?
                Should i do chunking manually before running the training script or is there some easy way to do this?
            A：See this section for the packing operation we used in the pre-training stage:
                https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt#preparing-the-dataset
        结论：pretrain时，训练集的长文，是被分段后进行训练，不涉及到cutoff_length的截断操作，最多只在长文最后一段（该段不够长时）才可能涉及到drop。

    2）关于训练刚开始时的overflow（参考：https://github.com/THUDM/CogVideo/issues/233）
        报错：
            [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
        建议：
            It is normal to skip when the loss is large at the beginning of training.
            You can find that a small number of steps will be skipped in the first 50 steps.
            Once the training is stable, it will not happen again.
    3）报错：Token indices sequence length is longer than the specified maximum sequence length for this model (159603 > 131072). Running this sequence through the model will result in indexing errors
        结论：根据issue中hiyouga反馈，不影响训练，也和cutoff length无关。（https://github.com/hiyouga/LLaMA-Factory/issues/5952）

5、数据相关
    1）parquet列式数据格式
        a）win下的parquet显示工具（https://github.com/mukunku/ParquetViewer/releases）
        b）parquet转jsonl
            见server/life-agent/tools/data_tool/parquet.py
    2)llama-factory中的dataset_info.json
        a)自定义的pt数据的引用要采用下面格式，可参考自带的c4_demo.json（要注意，否则llama-factory会默认是sft数据）
          "00": {
            "file_name": "/home/tutu/data/train-00000-of-00192.jsonl",
            "columns": {
              "prompt": "text"
            }
          },
          "01": {
            "file_name": "/home/tutu/data", （这样就可以训练data下的所有如jsonl文件，但是data下必须只有jsonl一种类型的文件，而且不能有其他文件夹包括.xxx这种文件夹，否则会报错）
            "columns": {
              "prompt": "text"
            }
          },

6、tensorboard
    1）将gpu_server/train_tools下的tensorboard.sh和trainer_log_to_tensorboard.py复制到如/home/tutu/LLaMA-Factory/saves/Qwen2.5-1.5B/full/cpt_all_2024-11-11-09-41-24下
    2）pip install tensorboard
    3）./tensorboard.sh
    4）打开网页，展开loss页面

附）2024-11-27训练情况：
    模型基座：/home/tutu/models/Qwen2.5-1.5B
    训练时间：16天3小时
    epoch：1
    loss：3.3->2.4左右
    steps：75810
    训练平均速度：18.4s/it

    训练参数：
    cutoff length	4096
    epoches		1
    max samples	10000000000
    learning rate	5e-5
    batch size		1
    deepspeed stage	3
    compute type	fp16
    saving steps	5000
    extra arguments	{"optim": "adamw_torch", "save_only_model":true}

    datasets		"all"
    数据集配置文件：/home/tutu/LLaMA-Factory/data/dataset_info.json
    {
      "all": {
        "file_name": "/home/tutu/data/xxx/jsonl",
        "columns": {
          "prompt": "text"
        }
      }
    }

7、关于训练的关键超参（【过拟合要优于欠拟合】）
    关于loss收敛：
        a）sft数据只有几十条时，epoch到几十以上，直到loss到3左右，loss小于8时才会开始回复部分训练数据、loss小于6时会精确复述一句话但如人名会错（如sft 4b-chat时，lr设置为5e-6, epoch设置为100则训练完成后的train_loss到3.3，可以完整复述一段话、人名也正确。）
        b）可能和显存有关：报warning，1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
    1）学习率
        SFT数据集不是特别大的情况下，建议设置较小学习率，一般设置为pre-train阶段学习率的0.1左右，如在pre-train阶段的学习率为5e-5，则SFT学习率设置为5e-6。
        在10万SFT样本上，采用与pre-train一样的学习率，发现loss一直不收敛，在调低学习率至原来0.1之后，loss在两个epoch之后就收敛。
    2）Epoch
        Epoch设置可以根据loss收敛情况设置，如果SFT样本较少，可以设置较大epoch，在较小的epoch上loss会不收敛，指令都很难遵循。较大epoch会容易导致过拟合，但过拟合要优于欠拟合。
        如果SFT样本数量较多，如在十万以上，一般2个epoch即可收敛。

8、（不好用，正常运行后会卡死）使用neo-map处理数据(MAP-NEO/Matrix/document-convert)
    0）下载
        git clone https://github.com/multimodal-art-projection/MAP-NEO
        cd MAP-NEO/Matrix/document-convert
    1）安装
        conda create -n neo python=3.10(FastDeploy之类最大支持3.10)
        conda activate neo
        pip install -r requirements.txt
        python -m pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html

        安装cuDNN：
            去 NVIDIA 官网下载对应版本的 cuDNN（https://developer.nvidia.com/cudnn）
            1）下载
                https://developer.download.nvidia.com/compute/cudnn/9.6.0/local_installers/cudnn-local-repo-ubuntu2204-9.6.0_1.0-1_amd64.deb
            sudo dpkg -i cudnn-local-repo-ubuntu2204-9.6.0_1.0-1_amd64.deb
            sudo cp /var/cudnn-local-repo-ubuntu2204-9.6.0/cudnn-*-keyring.gpg /usr/share/keyrings/
            sudo apt update
            sudo apt -y install cudnn

            找到安装的位置（如/usr/lib/x86_64-linux-gnu，可以用find /usr -name "libcudnn*"查找）
            echo 'export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH' >> ~/.bashrc
            echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc
            source ~/.bashrc

            如有权限问题
            sudo chmod 755 /usr/lib/x86_64-linux-gnu/libcudnn*
    2）运行
        运行前，把~/neo_data/下的无关文件、隐藏文件、隐藏文件夹全部删除
        python multi_thread_process_to_doc.py ~/neo_data/ --process-num 8
    3）可能的报错
        a）报错：ImportError: libcudart.so.11.0: cannot open shared object file: No such file or directory
        (尚未解决)解决：可能需要在ubuntu安装多版本cuda toolkit(https://blog.csdn.net/weixin_64174049/article/details/140065229?spm=1001.2014.3001.5506)

        b）报错：cannot import name 'xyxy24p' from 'cnstd.yolov7.general'
        解决：
            1）如下修改latex/latex_rec.py第24行：
# from cnstd.yolov7.general import xyxy24p, box_partial_overlap
from cnstd.yolov7.general import box_partial_overlap
            2）不要用pip install cnstd==1.2.3解决这个错误，会产生新问题

        c）报错：PreconditionNotMetError: Cannot load cudnn shared library. Cannot invoke method cudnnGetVersion.
        解决：安装cuDNN