1、硬件连接
    每个服务器有2个25Gbps的光纤模块，用2根光纤直连（不需要交换机，但需要注意tx和rx的对接）

2、安装mellanox网卡驱动
    下载和安装（无法直接wget，要tgz的链接点进去下载后上传至服务器）
        https://network.nvidia.com/products/infiniband-drivers/linux/mlnx_ofed/
        tar -xvf MLNX_OFED_LINUX-24.10-2.1.8.0-openeuler22.03sp3-x86_64.tar
        sudo yum install pciutils-devel tk fuse-devel
        cd MLNX_OFED_LINUX-24.10-2.1.8.0-openeuler22.03sp3-x86_64/
        sudo ./mlnxofedinstall
        sudo dracut -f
        sudo /etc/init.d/openibd restart(似乎不需要)
        sudo reboot
    测试
        ibstat
        ibv_devinfo（可以用来看网卡是IB模式还是RoCE模式）
            transport: InfiniBand            →  IB 模式，
            transport: iWARP or Ethernet     →  RoCE 模式（RoCEv1/v2）
        rdma link show
    设置IP（此时2个网卡都没有IP，以下设置方式似乎是临时设置方式）
        服务器A：
            sudo ip addr add 192.168.88.1/24 dev ens19f0np0
            sudo ip link set ens19f0np0 up
            sudo ip addr add 192.168.88.11/24 dev ens19f1np1
            sudo ip link set ens19f1np1 up
        服务器B：
            sudo ip addr add 192.168.88.2/24 dev ens19f0np0
            sudo ip link set ens19f0np0 up
            sudo ip addr add 192.168.88.22/24 dev ens19f1np1
            sudo ip link set ens19f1np1 up
    设置IB模式（似乎不行，也可能不需要）
        启动mellanox管理程序
            sudo modprobe mst_pci
            sudo mst start
        查看设备
            sudo mst status
            sudo mlxconfig -d /dev/mst/mt4127_pciconf0 q | grep LINK_TYPE（这里总是返回空）

3、设置ip(前面IB驱动装了，且ip设置了，这一步就不需要了)
    查看设备情况：
        ifconfig
    查看光纤网卡情况：
        lspci | grep -i ether
        lspci | grep -i infiniband（返回空）
        ethtool -i ens19f0np0
    服务器A：
        sudo ip addr add 192.168.88.1/24 dev ens19f0np0
        sudo ip link set ens19f0np0 up
        sudo ip addr add 192.168.88.11/24 dev ens19f1np1
        sudo ip link set ens19f1np1 up
    服务器B：
        sudo ip addr add 192.168.88.2/24 dev ens19f0np0
        sudo ip link set ens19f0np0 up
        sudo ip addr add 192.168.88.22/24 dev ens19f1np1
        sudo ip link set ens19f1np1 up
    测试：
        ping 192.168.88.2
        服务器B:
            iperf3 -s
        服务器A:
            iperf3 -c 192.168.88.2
            iperf3 -c 192.168.88.22
            可以测得25Gbps的速度

4、此时即可2个服务器之间快速copy（如从192.168.88.1复制sync_test文件夹到192.168.88.2）
    rsync -avz --checksum ./sync_test tutu@192.168.88.2:/home/tutu/sync_test

5、nccl通信检查
    1、（这条没用）没有交换机时，IB模式需要装opensm、Base lid不能为0（应为0x2之类）
    2、（这条没用）根据检测，cx6-lx网卡不能跑IB，只能跑RoCE（mlxconfig 回答 “The Device doesn't support LINK_TYPE_P1” 的根本原因，是 这类 EN 卡硬件只支持 Ethernet /RoCE，压根没有 InfiniBand 模式，所以也就没有 LINK_TYPE_P* 参数可改。官方论坛给过同样结论：只有 VPI 型号 才能切换端口类型；DX/Lx 等 ETH‑only 卡不能切换。产品手册也明确把 ConnectX‑6 Lx 定义为 Ethernet adapter cards）
        （但是前面的IB驱动需要，最后要确保2个服务器的NCCL_IB_HCA正确，可能是rocep39s0f0,rocep39s0f1，也可能是mlx5_0,mlx5_1，而NCCL_SOCKET_IFNAME只需要填一个网卡设备号如ens19f0np0即可（仅用于握手而非RDMA通信））
    3、可以用以下命令测试不用IB模式也不用RoCE模式（当普通25Gbps网卡用，只是延时是几十ms而不是几ms）
        export NCCL_IB_DISABLE=1
        export NCCL_SOCKET_IFNAME=ens19f0np0
        服务器A：torchrun --nproc-per-node=4 --nnodes=2 --node-rank=0 --master-addr=192.168.88.1 --master-port=28002 test.py
        服务器B：torchrun --nproc-per-node=4 --nnodes=2 --node-rank=1 --master-addr=192.168.88.1 --master-port=28002 test.py
-------------------------------test.py-------------------------------
import os
import torch
import torch.distributed as dist

def setup():
    dist.init_process_group(backend="nccl", init_method="env://")
    rank = dist.get_rank()
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)

    t = torch.tensor([rank], device='cuda')
    dist.all_reduce(t)
    print(f"[Rank {rank}] All-Reduced Value: {t.item()}")

if __name__ == "__main__":
    setup()
------------------------------/test.py-------------------------------
        test.py可以通过（此时即可用，后续RoCE或者IB模式可以延时再低一个数量级）

    4、以下命令用于检查2个服务器软件是否版本相同：
        ofed_info -s         # MLNX_OFED
        rpm -q rdma-core     # 发行版驱动
