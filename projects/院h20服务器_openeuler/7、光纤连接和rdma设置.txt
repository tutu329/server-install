1、硬件连接(根据检测，mellanox cx6-lx网卡不能跑IB，只能跑RoCE)
    每个服务器有2个25Gbps的光纤模块，用2根光纤直连（不需要交换机，但需要注意tx和rx的对接）

2、安装mellanox网卡驱动
    下载和安装（无法直接wget，要tgz的链接点进去下载后上传至服务器）
        https://network.nvidia.com/products/infiniband-drivers/linux/mlnx_ofed/
        tar -xvf MLNX_OFED_LINUX-24.10-2.1.8.0-openeuler22.03sp3-x86_64.tar
        sudo yum install pciutils-devel tk fuse-devel
        cd MLNX_OFED_LINUX-24.10-2.1.8.0-openeuler22.03sp3-x86_64/
        sudo ./mlnxofedinstall
        sudo dracut -f
        sudo /etc/init.d/openibd restart(似乎不需要)
        sudo reboot
    测试
        ibstat
        ibv_devinfo
        rdma link show
    设置IP（RoCE时2个服务器的设置mtu 9000（有说>4200即可）很重要，IB时设置mtu 4096）
        服务器A：
            sudo nmcli con add type ethernet ifname ens19f0np0 con-name ens19f0np0 ip4 192.168.88.1/24
            sudo nmcli con mod ens19f0np0 802-3-ethernet.mtu 9000
            sudo nmcli con up  ens19f0np0
                如果sudo nmcli con add报错有其他联接：
                    查看所有关联的uuid：nmcli -f NAME,UUID,TYPE,DEVICE,AUTOCONNECT connection show | grep ens19f0np0
                    删除所有关联：sudo nmcli connection delete uuid xxxx（如552b586d-02c7-410e-be0f-464a442d6f63）

            sudo nmcli con add type ethernet ifname ens19f1np1 con-name ens19f1np1 ip4 192.168.88.11/24
            sudo nmcli con mod ens19f1np1 802-3-ethernet.mtu 9000
            sudo nmcli con up  ens19f1np1
        服务器B：
            sudo nmcli con add type ethernet ifname ens19f0np0 con-name ens19f0np0 ip4 192.168.88.2/24
            sudo nmcli con mod ens19f0np0 802-3-ethernet.mtu 9000
            sudo nmcli con up  ens19f0np0

            sudo nmcli con add type ethernet ifname ens19f1np1 con-name ens19f1np1 ip4 192.168.88.22/24
            sudo nmcli con mod ens19f1np1 802-3-ethernet.mtu 9000
            sudo nmcli con up  ens19f1np1
        此时可以ping 192.168.88.2
    （RoCE卡不需要）设置IB模式（似乎不行，也可能不需要）
        启动mellanox管理程序
            sudo modprobe mst_pci
            sudo mst start
        查看设备
            sudo mst status
            sudo mlxconfig -d /dev/mst/mt4127_pciconf0 q | grep LINK_TYPE（这里总是返回空）

3、此时即可2个服务器之间快速copy（如从192.168.88.1复制sync_test文件夹到192.168.88.2）
    rsync -avz --checksum ./sync_test tutu@192.168.88.2:/home/tutu/sync_test

4、nccl通信检查（主要测试不用IB和RoCE、只用socket行不行）
    1、（这条没用）没有交换机时，IB模式需要装opensm、Base lid不能为0（应为0x2之类）
    2、（这条没用）根据检测，cx6-lx网卡不能跑IB，只能跑RoCE（mlxconfig 回答 “The Device doesn't support LINK_TYPE_P1” 的根本原因，是 这类 EN 卡硬件只支持 Ethernet /RoCE，压根没有 InfiniBand 模式，所以也就没有 LINK_TYPE_P* 参数可改。官方论坛给过同样结论：只有 VPI 型号 才能切换端口类型；DX/Lx 等 ETH‑only 卡不能切换。产品手册也明确把 ConnectX‑6 Lx 定义为 Ethernet adapter cards）
        （但是前面的IB驱动需要，最后要确保2个服务器的NCCL_IB_HCA正确，可能是rocep39s0f0,rocep39s0f1，也可能是mlx5_0,mlx5_1，而NCCL_SOCKET_IFNAME只需要填一个网卡设备号如ens19f0np0即可（仅用于握手而非RDMA通信））
    3、可以用以下命令测试不用IB模式也不用RoCE模式（当普通25Gbps网卡用，只是延时是几十ms而不是几ms）
        export NCCL_IB_DISABLE=1
        export NCCL_SOCKET_IFNAME=ens19f0np0
        sudo systemctl stop firewalld（这里非常关键！！！：注意重启后防火墙就会打开，另外这里主要是放行IB或RoCE的端口如49152–65535）
        服务器A：torchrun --nproc-per-node=4 --nnodes=2 --node-rank=0 --master-addr=192.168.88.1 --master-port=28002 test.py
        服务器B：torchrun --nproc-per-node=4 --nnodes=2 --node-rank=1 --master-addr=192.168.88.1 --master-port=28002 test.py
-------------------------------test.py-------------------------------
import os
import torch
import torch.distributed as dist

def setup():
    dist.init_process_group(backend="nccl", init_method="env://")
    rank = dist.get_rank()
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)

    t = torch.tensor([rank], device='cuda')
    dist.all_reduce(t)
    print(f"[Rank {rank}] All-Reduced Value: {t.item()}")

if __name__ == "__main__":
    setup()
------------------------------/test.py-------------------------------
        test.py可以通过（此时即可用，后续RoCE或者IB模式可以延时再低一个数量级）

    4、以下命令用于检查2个服务器软件是否版本相同：
        ofed_info -s         # MLNX_OFED
        rpm -q rdma-core     # 发行版驱动
    5、验证RDMA连通性
        sudo yum install -y perftest rdma-core
        服务器A：ib_write_bw -d mlx5_0 -F -x 3 192.168.88.2
        服务器B：ib_write_bw -d mlx5_0 -F -x 3
        服务器A和B的结果应为类似这样：
---------------------------------------------------------------------------------------
                    RDMA_Write BW Test
 Dual-port       : OFF          Device         : mlx5_0
 Number of qps   : 1            Transport type : IB
 Connection type : RC           Using SRQ      : OFF
 PCIe relax order: ON           Lock-free      : OFF
 ibv_wr* API     : ON           Using DDP      : OFF
 TX depth        : 128
 CQ Moderation   : 1
 Mtu             : 4096[B]
 Link type       : Ethernet
 GID index       : 3
 Max inline data : 0[B]
 rdma_cm QPs     : OFF
 Data ex. method : Ethernet
---------------------------------------------------------------------------------------
 local address: LID 0000 QPN 0x00a6 PSN 0x48ddea RKey 0x1ffebc VAddr 0x007f420e49d000
 GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:88:01
 remote address: LID 0000 QPN 0x00b9 PSN 0x624485 RKey 0x01dbbc VAddr 0x007f155f635000
 GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:88:02
---------------------------------------------------------------------------------------
 #bytes     #iterations    BW peak[MiB/sec]    BW average[MiB/sec]   MsgRate[Mpps]
 65536      5000             2920.76            2920.71              0.046731
---------------------------------------------------------------------------------------

6、RoCE的设置和测试（主要是解决NCCL的锁页内存 (memlock)问题）
    关于报错：ibv_reg_mr_iova2 failed with error Cannot allocate memory
        原因：
            为什么 NCCL 需要 “锁页内存 (memlock)”？
                RDMA 传输前必须把显存/内存注册给网卡（ibv_reg_mr），注册时会把这块内存“锁”在物理内存里，不能被换出。
                如果 memlock 限制太小，驱动申请时就会失败，于是你看到：
        (不需要了）临时解决：
            2个服务器都运行sudo prlimit --pid $$ --memlock=unlimited:unlimited
            sudo prlimit --pid $$ --memlock=unlimited:unlimited 到底做了什么？
                prlimit 是 Linux 自带的一个工具，用来在 进程运行期间 修改它的资源限制（等价于 C 里的 setrlimit()）。
                --pid $$ 指定要修改的进程就是你当前的 Shell（$$ 是当前 Shell 的 PID）。
                --memlock=软限制:硬限制 把 RLIMIT_MEMLOCK 的软、硬两级限制都设成 unlimited。
                加了 sudo 才能把「硬限制」也调高——普通用户只能降低硬限制，不能升高。
                结果就是：
                    当前 Shell 以及它启动的所有子进程（包括你随后运行的 torchrun）都拥有“无限制的锁页内存”；
                    一旦退出这个 Shell，设置就消失；打开新终端还是默认的 64 MiB。
        最终解决：
            1、执行下述命令
sudo tee /etc/security/limits.d/99-nccl.conf <<'EOF'
* soft memlock unlimited
* hard memlock unlimited
EOF
                此时用termius新登陆ssh，ulimit -l都是返回unlimited了
            2、解决jupyter仍然返回65536的问题（因为jupyter启动时继承自systemd而不是PAM，而Termius→sshd→pam_limits.so）
sudo mkdir -p /etc/systemd/system.conf.d
sudo tee /etc/systemd/system.conf.d/90-memlock.conf <<'EOF'
[Manager]
DefaultLimitMEMLOCK=infinity
EOF
                重启后，jupyter就能返回unlimit
            3、最终start_test.sh为
# export NCCL_IB_DISABLE=1
# HCA是设置RDMA通道，所以要设置mlx5_0和mlx5_1
export NCCL_IB_HCA=mlx5_0,mlx_1
# socket_ifname只是设置握手设备，所以只需要ens19f0np0即可
export NCCL_SOCKET_IFNAME=ens19f0np0
# GID_INDEX设置为3是因为旧版本 NCCL（<2.21）不会自己去“猜”哪个 GID 能通，为了解决系统误用INDEX 0的问题，执行show_gids可以看到下面内容(index 0和1为ipv6,4)：
# 【因此NCCL_IB_GID_INDEX要设置为show_gids结果中IPv4有的IP的V2那个INDEX.【但是实测5就是会有问题，3没问题，也许show_gids有刷新问题】】
# DEV     PORT    INDEX   GID                                     IPv4            VER     DEV
# ---     ----    -----   ---                                     ------------    ---     ---
# mlx5_0  1       0       fe80:0000:0000:0000:5aa2:e1ff:feb0:3772                 v1      ens19f0np0
# mlx5_0  1       1       fe80:0000:0000:0000:5aa2:e1ff:feb0:3772                 v2      ens19f0np0
# mlx5_0  1       2       fe80:0000:0000:0000:a789:516a:e270:5b33                 v1      ens19f0np0
# mlx5_0  1       3       fe80:0000:0000:0000:a789:516a:e270:5b33                 v2      ens19f0np0
# mlx5_0  1       4       0000:0000:0000:0000:0000:ffff:c0a8:5801 192.168.88.1    v1      ens19f0np0
# mlx5_0  1       5       0000:0000:0000:0000:0000:ffff:c0a8:5801 192.168.88.1    v2      ens19f0np0
# n_gids_found=6
export NCCL_IB_GID_INDEX=3
sudo systemctl stop firewalld
source /home/tutu/miniconda3/etc/profile.d/conda.sh
conda activate sgl45
torchrun --nproc-per-node=4 --nnodes=2 --node-rank=1 --master-addr=192.168.88.1 --master-port=28002 test.py
            此时运行start_test.sh，test.py就可正常返回all_reduce结果了

7、多节点llama-factory训练
    NCCL_IB_DISABLE=0
    NCCL_P2P_DISABLE=0
    NCCL_SHM_DISABLE=0

    NCCL_IB_GID_INDEX=3
    NCCL_IB_HCA=mlx5_0,mlx5_1
    NCCL_IB_TC=106
    NCCL_SOCKET_IFNAME=ens19f0np0

    服务器A：
        FORCE_TORCHRUN=1 NNODES=2 NODE_RANK=0 MASTER_ADDR=192.168.88.1 MASTER_PORT=28011 \
            llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
    服务器B：
        FORCE_TORCHRUN=1 NNODES=2 NODE_RANK=1 MASTER_ADDR=192.168.88.1 MASTER_PORT=28011 \
            llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml

(没用，openmpi用ib，我们是RoCE卡)7、NCCL-test测试(2个服务器上都要装)
    安装nccl：
        https://developer.nvidia.com/nccl/nccl-download，截图给gpt4看，问open-euler具体版本（cat /etc/os-release）用哪个
        sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo
        sudo dnf clean all
        sudo dnf makecache

        # 安装 NCCL 2.26.2 + CUDA 12.8 对应版本
        sudo dnf install \
            libnccl-2.26.2-1+cuda12.8 \
            libnccl-devel-2.26.2-1+cuda12.8 \
            libnccl-static-2.26.2-1+cuda12.8
    安装MPI
        sudo dnf install -y openmpi openmpi-devel
        mpicc --version
        找到mpi.h: find /usr -name mpi.h 2>/dev/null
        vi ~/.bashrc
export CPATH=/usr/include/openmpi-x86_64:$CPATH
export PATH=/usr/lib64/openmpi/bin:$PATH
export LD_LIBRARY_PATH=/usr/lib64/openmpi/lib:$LD_LIBRARY_PATH
export LIBRARY_PATH=/usr/lib64/openmpi/lib:$LIBRARY_PATH
        source ~/.bashrc

    安装编译nccl-test
        git clone https://github.com/NVIDIA/nccl-tests.git
        cd nccl-tests
        make MPI=1
    服务器A设置SSH
        ssh-keygen（全部回车）
        ssh-copy-id tutu@192.168.88.2
        测试：ssh 192.168.88.2 hostname
    测试nccl-test（服务器A上运行即可）
        vi nccl-test.sh
#!/bin/bash

# ===== 用户配置 =====
HOST1=192.168.88.1
HOST2=192.168.88.2
GPUS_PER_NODE=8

# RoCE 相关设置
GID_INDEX=3
HCA_LIST="mlx5_0,mlx5_1"   # 👉 同时使用两个 HCA，如果只用一个就写 mlx5_1

# 网络接口名，用于 NCCL_SOCKET_IFNAME（ifconfig 或 ip a 看你 RoCE 对应的是哪个）
IFNAME=ens19f0np0

# ===== NCCL + RDMA 环境变量 =====
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=0
export NCCL_IB_GID_INDEX=$GID_INDEX
export NCCL_IB_HCA=$HCA_LIST
export NCCL_NET_GDR_LEVEL=5
export NCCL_SOCKET_IFNAME=$IFNAME
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64:/usr/lib64/openmpi/lib

# ===== 运行 NCCL 测试 =====
mpirun -np $((GPUS_PER_NODE * 2)) \
  -H ${HOST1}:${GPUS_PER_NODE},${HOST2}:${GPUS_PER_NODE} \
  -x NCCL_DEBUG \
  -x NCCL_IB_DISABLE \
  -x NCCL_IB_GID_INDEX \
  -x NCCL_IB_HCA \
  -x NCCL_NET_GDR_LEVEL \
  -x NCCL_SOCKET_IFNAME \
  -x LD_LIBRARY_PATH \
  ./build/all_reduce_perf -b 8 -e 512M -f 2 -g ${GPUS_PER_NODE}




