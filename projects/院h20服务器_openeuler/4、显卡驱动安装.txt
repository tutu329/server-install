1、安装依赖
    更换yun的国内镜像
        sudo cp /etc/yum.repos.d/openEuler.repo /etc/yum.repos.d/openEuler.repo.bak
        sudo sed -i 's|http://repo.openeuler.org|https://mirrors.aliyun.com/openeuler|g' /etc/yum.repos.d/openEuler.repo
        sudo yum clean all
        sudo yum makecache
    sudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) dkms gcc make tar gzip
2、设置
    setenforce 0
    sudo vi /etc/selinux/config(SELINUX=enforcing改为SELINUX=disabled)
3、下载安装cuda toolkit(包含了驱动和cuda运行库)
    1)https://developer.nvidia.com/cuda-downloads选择: Linux->x86_64->KylinOS->10->runfile(local)
    2)wget https://developer.download.nvidia.com/compute/cuda/12.8.1/local_installers/cuda_12.8.1_570.124.06_linux.run
    3)sudo sh cuda_12.8.1_570.124.06_linux.run
        1）输入accept。
        2）直接选择install。（注意不要安装nvidia-fs，否则好像安装失败（是需要安装一些前置程序））
        3）安装中每一步的等待时间都比较长。
    4)vi ~/.bashrc(在"export PATH"这一行后面增加)
        export PATH=/usr/local/cuda-12.8/bin:$PATH
        export LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$LD_LIBRARY_PATH
    5)source ~/.bashrc
    6)nvcc --version和nvidia-smi即可ok
    7)设置显卡persistence模式
        sudo nvidia-persistenced --persistence-mode  # 启动守护进程
sudo tee /etc/systemd/system/nvidia-persistenced.service <<EOF
[Unit]
Description=NVIDIA Persistence Daemon
Wants=syslog.target

[Service]
Type=forking
ExecStart=/usr/bin/nvidia-persistenced --user root
ExecStopPost=/bin/rm -rf /var/run/nvidia-persistenced
Restart=always

[Install]
WantedBy=multi-user.target
EOF
        sudo systemctl enable nvidia-persistenced    # 设为开机自启
        sudo systemctl daemon-reload
        sudo systemctl enable --now nvidia-persistenced
        sudo systemctl status nvidia-persistenced
4、查看功耗
    1)sudo yum install ipmitool
    2)查看sensor: sudo ipmitool sensor
    3)查看功耗: sudo ipmitool sdr | grep -i watt
    4)nvidia-smi查看nvlink情况: nvidia-smi topo -m

5、关于torch.cuda.is_available()报错，是因为sxm服务器必须启动nvidia-fabricmanager才行
    # 1. 写入 Kylin10 repo 文件
    sudo wget -O /etc/yum.repos.d/cuda-kylin10.repo https://developer.download.nvidia.com/compute/cuda/repos/kylin10/x86_64/cuda-kylin10.repo
    # 2. 导入 NVIDIA 公钥
    sudo rpm --import https://developer.download.nvidia.com/compute/cuda/repos/kylin10/x86_64/D42D0685.pub
    # 3. 刷新缓存
    sudo dnf clean all && sudo dnf makecache
    # 安装与驱动同版号的 Fabric Manager和Persistenced（版本号用nvidia-smi查看，左上角）
    sudo dnf install -y nvidia-driver-libs-570.124.06
    sudo dnf install -y nvidia-fabric-manager-570.124.06 nvidia-persistenced-570.124.06
    sudo systemctl enable --now nvidia-fabricmanager
    sudo systemctl enable --now nvidia-persistenced
    验证：
        systemctl status nvidia-fabricmanager
        nvidia-smi nvlink -s         # NVSwitch 状态应为 Active
        安装torch后的python验证：
python - <<'EOF'
import torch, subprocess, os, platform
print("torch:", torch.__version__, "cuda libs:", torch.version.cuda)
print("is_available:", torch.cuda.is_available())
print("device_count:", torch.cuda.device_count())
EOF
    成功！

6、sglang推理deepseek r1需要g++
    sudo dnf install gcc-c++

7、sglang推理r1用NextN
    git clone https://github.com/sgl-project/sglang
    cd sglang/scripts
    python export_deepseek_nextn.py --input-dir ~/models/DeepSeek-R1 --output-dir ~/models/DeepSeek-R1-NextN

8、sglang推理r1的指令(1*H20单线程为30t/s)
    export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    export SGL_ENABLE_JIT_DEEPGEMM=1
    source /home/tutu/miniconda3/etc/profile.d/conda.sh
    conda activate sgl44
    python -m sglang.launch_server --served-model-name=ds-r1-671b --model-path=/home/tutu/models/DeepSeek-R1 \
    --enable-p2p-check --reasoning-parser=deepseek-r1 --trust-remote-code --host=0.0.0.0 --port=28001 \
    --mem-fraction-static=0.9 \
    --random-seed=446369220 \
    --tp=8 --max-total-tokens=65000 --max-running-requests=64 \
    --disable-radix \
    # --enable-torch-compile --torch-compile-max-bs 1 \         # (开torch compile会oom)
    --enable-ep-moe \
    --speculative-algo NEXTN \
    --speculative-draft /home/tutu/models/DeepSeek-R1-NextN \
    --speculative-num-steps 2 \
    --speculative-eagle-topk 4 \
    --speculative-num-draft-tokens 4

9、tuning fused moe triton（30t/s->35t/s）
    参考：
        1)https://github.com/sgl-project/sglang/issues/3956#user-content-fnref-11-e81d04ca2b83568d349df11d65f5dc17
        @lishicheng1996 May be you need tuning fused moe kernel on H20 with scripts then copy config to moe configs. because default configs not included H20
    Thanks for you reply! We did tuning the fused_moe config, and the speed improvement is about 1.2 tokens/s~ May I know your tuning improvement and the test query? Thank you very much~
    Sorry, We did not separately calculate the improvement of moe tuning. I test serving use python3 -m sglang.bench_serving , random dataset, /v1/completions endpoint, input 1k, output 1k
    We found that tuning fused_moe config can have a improvement about 16 tokens/s (38->54 tokens/s) with NEXTN, although it has little gain on no-speculative decoding.
        2)readme: https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton

    具体操作：
        git clone https://github.com/sgl-project/sglang
        cd sglang
        conda activate sgl44
        pip install ray
        关闭所有gpu应用，nvidia-smi查看gpu都空载
        然后运行下面指令（dtype查看tuning_fused_moe.py确认，目前可选"fp8_w8a8"、"int8_w8a16"、"int8_w8a8"），可能需要几个小时。
            python benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py \
                --model /home/tutu/models/DeepSeek-R1 \
                --tp-size 8 \
                --dtype fp8_w8a8 \
                --tune
            关于报错：ImportError: cannot import name 'fused_experts' from partially initialized module
                解决办法：
                    conda activate sglang44
                    pip install sglang -U
        然后获得tuning生成在当前目录下的json文件('E=256,N=256,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128, 128].json')
        复制到conda环境对应的sglang下。(如/home/tutu/miniconda3/envs/sgl44/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/)
            cp 'E=256,N=256,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128, 128].json' /home/tutu/miniconda3/envs/sgl44/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/
        然后运行sglang

10、关于安装ds的官方flash_mla（似乎只有H800有用）
    git clone https://github.com/deepseek-ai/FlashMLA
    cd FlashMLA
    python setup.py install