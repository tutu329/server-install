=================================一、pip包下载到内网=================================
pip download 打一包，内网用 --no-index 安装
在一台能上网的机器上：

-----------------------------------------linux打包-----------------------------------------
# 1) 建虚拟环境（可选，但推荐）
python3 -m venv venv && source venv/bin/activate
python -m pip install -U pip wheel

# 2) 写一个需求清单（版本可按需固定）
cat > requirements.txt << 'EOF'
openai            # 建议固定版本：如 openai==1.**.*
openai-agents     # 建议固定版本：如 openai-agents==0.**.*
jiter>=0.4,<1
mcp<2,>=1.11.0
EOF

# 3) 下载所有依赖到本地目录（纯二进制优先，避免到内网再编译）
mkdir -p wheelhouse
python -m pip download --only-binary=:all: -r requirements.txt -d wheelhouse

# 4) 打包带走
tar czf openai-wheelhouse-$(date +%Y%m%d).tar.gz wheelhouse requirements.txt

-----------------------------------------mac打包-----------------------------------------
# 1) 生成 requirements.txt（注意最后那一行必须是只有 EOF 本身，没有空格）
cat > requirements.txt <<'EOF'
openai            # 建议固定版本：如 openai==1.**.*
openai-agents     # 建议固定版本：如 openai-agents==0.**.*
jiter>=0.4,<1
mcp<2,>=1.11.0
EOF

# 2) 下载依赖（建议用 python3 -m pip）（注意python版本，如310）
mkdir -p wheelhouse
pip download --only-binary=:all: \
  --platform manylinux2014_x86_64 \
  --implementation cp \
  --python-version 310 \
  --abi cp310 \
  -r requirements.txt -d wheelhouse

# 3) 打包：避免直接用 $(...)，先存到变量里
tar -czf openai-wheelhouse-manylinux-py310-$(date +%Y%m%d).tar.gz wheelhouse requirements.txt

# 如果第 2 步因为没有纯二进制轮子而报错，就先去掉 --only-binary=:all: 再执行一次下载。

-----------------------------------------内网解包-----------------------------------------
# 1) 解包
mkdir -p ~/wheels && tar xzf openai-wheelhouse-*.tar.gz -C ~/wheels

# 2) 安装（完全离线，不访问外网）
python3 -m pip install --no-index \
  --find-links=~/wheels/wheelhouse \
  -r ~/wheels/requirements.txt

============二、chat测试：chat.completions和response(vllm-gptoss推理的gpt-oss-20b测试【正常】，vllm默认参数启动、未设置--tool-call-parser和--enable-auto-tool-choice)=============
from openai import OpenAI

client = OpenAI(
    base_url="http://10.0.10.6:28001/v1",
    api_key="EMPTY"
)

result = client.chat.completions.create(
    model="gpt-oss-20b",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "你是谁？"}
    ]
)

print(result.choices[0].message.content)

response = client.responses.create(
    model="gpt-oss-20b",
    instructions="You are a helfpul assistant.",
    input="你是谁？"
)

print(response.output_text)

# -----------------------------三、chat.completions调用tool(vllm-gptoss推理的gpt-oss-20b测试【不正常】、未调用工具，vllm默认参数启动、未设置--tool-call-parser和--enable-auto-tool-choice)-----------------------------------
from openai import OpenAI

client = OpenAI(
    base_url="http://10.0.10.6:28001/v1",
    api_key="EMPTY"
)

tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather in a given city",
            "parameters": {
                "type": "object",
                "properties": {"city": {"type": "string"}},
                "required": ["city"]
            },
        },
    }
]

response = client.chat.completions.create(
    model="gpt-oss-20b",
    messages=[{"role": "user", "content": "What's the weather in Berlin right now?"}],
    tools=tools
)

print(response.choices[0].message)

# --------------------------四、response(harmony)调用tool(vllm-gptoss推理的gpt-oss-20b测试【正常】，vllm默认参数启动、未设置--tool-call-parser和--enable-auto-tool-choice)--------------------------------
# response_tool.py
from openai import OpenAI
import json
import pprint

# 如走你内网 vLLM 网关，保持这样
client = OpenAI(base_url="http://10.0.10.6:28001/v1", api_key="EMPTY")
# 如直连官方：client = OpenAI()

# ✅ Responses 的“扁平”工具定义
tools = [
    {
        "type": "function",
        "name": "get_weather",
        "description": "Get current weather in a given city",
        "parameters": {
            "type": "object",
            "properties": {"city": {"type": "string"}},
            "required": ["city"],
            "additionalProperties": False,
        },
    }
]

# 1) 用户输入（Harmony/Responses：input 里放消息，用户文本用 input_text）
input_msgs = [
    {
        "role": "user",
        "content": [
            {"type": "input_text", "text": "What's the weather in Berlin right now?"}
        ],
    }
]

# 2) 第一次调用：让模型决定是否调用工具
resp = client.responses.create(
    model="gpt-oss-20b",
    input=input_msgs,
    tools=tools,
    tool_choice="auto",
)

print('[resp]')
pprint.pprint(resp)
print()

# 工具调用提取（兼容不同服务端返回）
def collect_function_calls(r):
    calls = []
    for item in getattr(r, "output", []) or []:
        if getattr(item, "type", None) in ("function_call", "tool_call"):
            calls.append(item)
    return calls

calls = collect_function_calls(resp)

print('[calls]')
pprint.pprint(calls )
print()

# 如果模型直接给出答案（无工具调用）
if not calls and getattr(resp, "output_text", None):
    print(resp.output_text)
else:
    # 3) 本地实现工具
    def get_weather(city: str):
        # 这里替换为真实实现
        return {"city": city, "temperature_c": 23, "status": "sunny"}

    # 4) 把每个工具结果作为一个“function_call_output”条目追加到 input 列表
    for call in calls:
        name = getattr(call, "name", None)
        call_id = getattr(call, "call_id", None) or getattr(call, "id", None)
        args = getattr(call, "arguments", {}) or {}
        if isinstance(args, str):
            args = json.loads(args)

        result = get_weather(**args)

        print('[result]')
        pprint.pprint(result)
        print()

        # ❗不要用 role:"tool"；用独立条目 function_call_output
        input_msgs.append({
            "type": "function_call_output",
            "call_id": call_id,
            "output": json.dumps(result),
        })

# --------------------------------需查看结果是否有tool_calls--------------------------------------



===========================五、agent-sdk(vllm-gptoss推理的gpt-oss-20b测试【正常】，vllm默认参数启动、未设置--tool-call-parser和--enable-auto-tool-choice)===========================
# uv pip install openai-agents
#
import asyncio
from openai import AsyncOpenAI
from agents import Agent, Runner, function_tool, OpenAIResponsesModel, set_tracing_disabled

set_tracing_disabled(True)

@function_tool
def get_weather(city: str) -> str:
    print(f"[debug] getting weather for {city}")
    return f"The weather in {city} is sunny."

async def main():
    agent = Agent(
        name="Assistant",
        instructions="You only respond in haikus.",
        model=OpenAIResponsesModel(
            model="gpt-oss-20b",
            openai_client=AsyncOpenAI(
                base_url="http://10.0.10.6:28001/v1",
                api_key="EMPTY",
            ),
        ),  # ← 这里需要这个逗号
        tools=[get_weather],
    )

    result = await Runner.run(agent, "What's the weather in Tokyo?")
    print(result.final_output)  # 如果版本不对，可试试 result.final_output_text

if __name__ == "__main__":
    asyncio.run(main())
